"""
Tests for the Bayesian model comparison module.

This module contains tests for the BayesianModelComparison class and related
utility functions for model selection and comparison.
"""

from typing import Dict, List, Optional, Tuple, Union
from unittest.mock import MagicMock, patch

import numpy as np
import pandas as pd
import pyro
import pytest
import torch

from pyrovelocity.models.modular.comparison import (
    BayesianModelComparison,
    ComparisonResult,
    create_comparison_table,
    select_best_model,
)
from pyrovelocity.models.modular.components.base import (
    BaseDynamicsModel,
    BaseInferenceGuide,
    BaseLikelihoodModel,
    BaseObservationModel,
    BasePriorModel,
)
from pyrovelocity.models.modular.model import ModelState, PyroVelocityModel


# Mock implementations for testing
class MockDynamicsModel(BaseDynamicsModel):
    """Mock dynamics model for testing."""

    def __init__(self, name="mock_dynamics_model"):
        super().__init__(name=name)
        self.state = {}

    def forward(self, context):
        """Forward pass that adds predictions to the context."""
        # Extract parameters from context
        parameters = context.get("parameters", {})

        # Add predictions to context
        context["predictions"] = torch.ones((10, 5)) * parameters.get(
            "alpha", 1.0
        )

        return context

    def _forward_impl(
        self,
        u: torch.Tensor,
        s: torch.Tensor,
        alpha: torch.Tensor,
        beta: torch.Tensor,
        gamma: torch.Tensor,
        scaling: Optional[torch.Tensor] = None,
        t: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Implementation of the forward method."""
        # Simple implementation for testing
        u_pred = torch.ones_like(u) * alpha
        s_pred = torch.ones_like(s) * alpha
        return u_pred, s_pred

    def _steady_state_impl(
        self,
        alpha: torch.Tensor,
        beta: torch.Tensor,
        gamma: torch.Tensor,
        scaling: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Implementation of the steady state calculation for testing."""
        # For testing, just return ones
        u_ss = torch.ones_like(alpha)
        s_ss = torch.ones_like(alpha)

        # Apply scaling if provided
        if scaling is not None:
            u_ss = u_ss * scaling
            s_ss = s_ss * scaling

        return u_ss, s_ss

    def _predict_future_states_impl(
        self,
        current_state: Tuple[torch.Tensor, torch.Tensor],
        time_delta: torch.Tensor,
        alpha: torch.Tensor,
        beta: torch.Tensor,
        gamma: torch.Tensor,
        scaling: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Implementation of the predict_future_states method."""
        # Simple implementation for testing
        u_current, s_current = current_state
        u_future = u_current + alpha * time_delta
        s_future = s_current + beta * time_delta
        return u_future, s_future


class MockLikelihoodModel(BaseLikelihoodModel):
    """Mock likelihood model for testing."""

    def __init__(self, name="mock_likelihood_model"):
        super().__init__(name=name)
        self.state = {}

    def forward(self, context):
        """Forward pass that just returns the input context."""
        return context

    def _log_prob_impl(self, observations, predictions, scale_factors=None):
        """Implementation of log probability calculation."""
        # Simple log probability calculation for testing
        # Convert inputs to PyTorch tensors if needed
        if not isinstance(observations, torch.Tensor):
            observations = torch.tensor(observations, dtype=torch.float32)
        if not isinstance(predictions, torch.Tensor):
            predictions = torch.tensor(predictions, dtype=torch.float32)

        return -torch.sum((observations - predictions) ** 2, dim=1)

    def _sample_impl(self, predictions, scale_factors=None):
        """Implementation of sampling."""
        # Return the predictions with some noise for testing
        if not isinstance(predictions, torch.Tensor):
            predictions = torch.tensor(predictions, dtype=torch.float32)

        return predictions + torch.randn_like(predictions) * 0.1


class MockPriorModel(BasePriorModel):
    """Mock prior model for testing."""

    def __init__(self, name="mock_prior_model"):
        super().__init__(name=name)
        self.state = {}

    def forward(self, context):
        """Forward pass that adds parameters to the context."""
        # Add parameters to context
        context["alpha"] = 1.0
        context["beta"] = 1.0
        context["gamma"] = 1.0

        return context

    def _register_priors_impl(self, prefix=""):
        """Implementation of prior registration."""
        # No-op for testing
        pass

    def _sample_parameters_impl(self, prefix=""):
        """Implementation of parameter sampling."""
        # Return empty dict for testing
        return {}


class MockObservationModel(BaseObservationModel):
    """Mock observation model for testing."""

    def __init__(self, name="mock_observation_model"):
        super().__init__(name=name)
        self.state = {}

    def forward(self, context):
        """Forward pass that adds observations to the context."""
        # Add observations to context
        context["observations"] = context.get("x", torch.ones((10, 5)))

        return context

    def _forward_impl(self, context):
        """Implementation of the forward method."""
        # Add observations to context
        context["observations"] = context.get("x", torch.ones((10, 5)))
        return context

    def _prepare_data_impl(self, adata, **kwargs):
        """Implementation of data preparation."""
        # Return empty dict for testing
        return {}

    def _create_dataloaders_impl(self, data, **kwargs):
        """Implementation of dataloader creation."""
        # Return empty dict for testing
        return {}

    def _preprocess_batch_impl(self, batch):
        """Implementation of batch preprocessing."""
        # Return the input batch for testing
        return batch


class MockGuideModel(BaseInferenceGuide):
    """Mock guide model for testing."""

    def __init__(self, name="mock_guide_model"):
        super().__init__(name=name)
        self.state = {}

    def forward(self, context):
        """Forward pass that just returns the input context."""
        return context

    def guide(self, x=None, time_points=None, **kwargs):
        """Mock guide function that accepts torch tensors.

        This method is used by the PyroVelocityModel.guide method.

        Args:
            x: Input data tensor (can be any tensor-like object)
            time_points: Time points tensor (can be any tensor-like object)
            **kwargs: Additional keyword arguments

        Returns:
            Empty dictionary for testing
        """
        # Just return an empty dict for testing
        return {}

    def __call__(self, model, *args, **kwargs):
        """Create a guide function for the given model."""
        self._model = model

        def guide(*args, **kwargs):
            """Mock guide function."""
            return {}

        return guide

    def _setup_guide_impl(self, model, **kwargs):
        """Implementation of guide setup."""
        pass

    def _sample_posterior_impl(self, **kwargs):
        """Implementation of posterior sampling."""
        # Return empty dict for testing
        return {}


@pytest.fixture
def sample_data():
    """Create sample data for testing."""
    # Create random data
    torch.manual_seed(42)
    n_cells = 10
    n_genes = 5

    # Generate random data with PyTorch
    x = torch.randn((n_cells, n_genes))
    observations = torch.abs(torch.randn((n_cells, n_genes)))
    time_points = torch.linspace(0, 1, 10)

    return {
        "x": x,
        "observations": observations,
        "time_points": time_points,
    }


@pytest.fixture
def mock_model(sample_data):
    """Create a mock PyroVelocityModel for testing."""
    # Create component models
    dynamics_model = MockDynamicsModel()
    prior_model = MockPriorModel()
    likelihood_model = MockLikelihoodModel()
    observation_model = MockObservationModel()
    guide_model = MockGuideModel()

    # Create PyroVelocityModel
    model = PyroVelocityModel(
        dynamics_model=dynamics_model,
        prior_model=prior_model,
        likelihood_model=likelihood_model,
        observation_model=observation_model,
        guide_model=guide_model,
    )

    return model


@pytest.fixture
def mock_posterior_samples():
    """Create mock posterior samples for testing."""
    # Create random samples with PyTorch
    torch.manual_seed(42)
    num_samples = 100
    n_genes = 5

    return {
        "alpha": torch.abs(torch.randn((num_samples, n_genes))),
        "beta": torch.abs(torch.randn((num_samples, n_genes))),
        "gamma": torch.abs(torch.randn((num_samples, n_genes))),
    }


@pytest.fixture
def comparison_instance():
    """Create a BayesianModelComparison instance for testing."""
    return BayesianModelComparison()


def test_comparison_result_initialization():
    """Test initialization of ComparisonResult."""
    # Create a simple ComparisonResult
    values = {"model1": 100.0, "model2": 110.0}
    result = ComparisonResult(metric_name="WAIC", values=values)

    # Check attributes
    assert result.metric_name == "WAIC"
    assert result.values == values
    assert result.differences is None
    assert result.standard_errors is None
    assert result.metadata is None


def test_comparison_result_best_model():
    """Test best_model method of ComparisonResult."""
    # Test for information criteria (lower is better)
    waic_values = {"model1": 100.0, "model2": 110.0, "model3": 90.0}
    waic_result = ComparisonResult(metric_name="WAIC", values=waic_values)
    assert waic_result.best_model() == "model3"

    # Test for Bayes factors (higher is better)
    bf_values = {"model1": 2.0, "model2": 0.5, "model3": 1.0}
    bf_result = ComparisonResult(metric_name="Bayes Factor", values=bf_values)
    assert bf_result.best_model() == "model1"


def test_comparison_result_to_dataframe():
    """Test to_dataframe method of ComparisonResult."""
    # Create a ComparisonResult with standard errors
    values = {"model1": 100.0, "model2": 110.0}
    standard_errors = {"model1": 5.0, "model2": 6.0}
    result = ComparisonResult(
        metric_name="WAIC",
        values=values,
        standard_errors=standard_errors,
    )

    # Convert to DataFrame
    df = result.to_dataframe()

    # Check DataFrame structure
    assert isinstance(df, pd.DataFrame)
    assert list(df.columns) == ["model", "WAIC", "WAIC_se"]
    assert len(df) == 2
    assert set(df["model"]) == {"model1", "model2"}


@patch("arviz.waic")
@patch.object(BayesianModelComparison, "_extract_log_likelihood")
def test_compute_waic(
    mock_extract_log_likelihood,
    mock_waic,
    comparison_instance,
    mock_model,
    mock_posterior_samples,
    sample_data,
):
    """Test compute_waic method."""
    # Mock _extract_log_likelihood to return PyTorch tensors
    mock_log_likes = torch.ones((100, 10))
    mock_extract_log_likelihood.return_value = mock_log_likes

    # Mock arviz.waic return value
    mock_waic_result = MagicMock()
    mock_waic_result.waic = 100.0
    mock_waic_result.pointwise = np.array([10.0, 20.0, 30.0, 40.0])
    mock_waic.return_value = mock_waic_result

    # Test without pointwise values
    waic = comparison_instance.compute_waic(
        model=mock_model,
        posterior_samples=mock_posterior_samples,
        data=sample_data,
        pointwise=False,
    )
    assert waic == 100.0

    # Test with pointwise values
    waic, pointwise = comparison_instance.compute_waic(
        model=mock_model,
        posterior_samples=mock_posterior_samples,
        data=sample_data,
        pointwise=True,
    )
    assert waic == 100.0
    assert isinstance(pointwise, torch.Tensor)
    assert len(pointwise) == 4


@patch("arviz.loo")
@patch.object(BayesianModelComparison, "_extract_log_likelihood")
def test_compute_loo(
    mock_extract_log_likelihood,
    mock_loo,
    comparison_instance,
    mock_model,
    mock_posterior_samples,
    sample_data,
):
    """Test compute_loo method."""
    # Mock _extract_log_likelihood to return PyTorch tensors
    mock_log_likes = torch.ones((100, 10))
    mock_extract_log_likelihood.return_value = mock_log_likes

    # Mock arviz.loo return value
    mock_loo_result = MagicMock()
    mock_loo_result.loo = 100.0
    mock_loo_result.pointwise = np.array([10.0, 20.0, 30.0, 40.0])
    mock_loo.return_value = mock_loo_result

    # Test without pointwise values
    loo = comparison_instance.compute_loo(
        model=mock_model,
        posterior_samples=mock_posterior_samples,
        data=sample_data,
        pointwise=False,
    )
    assert loo == 100.0

    # Test with pointwise values
    loo, pointwise = comparison_instance.compute_loo(
        model=mock_model,
        posterior_samples=mock_posterior_samples,
        data=sample_data,
        pointwise=True,
    )
    assert loo == 100.0
    assert isinstance(pointwise, torch.Tensor)
    assert len(pointwise) == 4


@patch.object(BayesianModelComparison, "_compute_log_marginal_likelihood")
def test_compute_bayes_factor(
    mock_compute_lml, comparison_instance, mock_model, sample_data
):
    """Test compute_bayes_factor method."""
    # Mock _compute_log_marginal_likelihood return values
    mock_compute_lml.side_effect = [
        -10.0,
        -12.0,
    ]  # log(ml1) = -10, log(ml2) = -12

    # Create a second mock model
    mock_model2 = mock_model

    # Compute Bayes factor
    bf = comparison_instance.compute_bayes_factor(
        model1=mock_model,
        model2=mock_model2,
        data=sample_data,
    )

    # Check result: exp(-10 - (-12)) = exp(2) â‰ˆ 7.39
    assert np.isclose(bf, np.exp(2), rtol=1e-5)


@patch("arviz.waic")
@patch.object(BayesianModelComparison, "_extract_log_likelihood")
def test_compare_models_waic(
    mock_extract_log_likelihood,
    mock_waic,
    comparison_instance,
    mock_model,
    mock_posterior_samples,
    sample_data,
):
    """Test compare_models method with WAIC."""
    # Mock _extract_log_likelihood to return PyTorch tensors
    mock_log_likes = torch.ones((100, 10))
    mock_extract_log_likelihood.return_value = mock_log_likes

    # Mock arviz.waic return values
    mock_waic_result1 = MagicMock()
    mock_waic_result1.waic = 100.0
    mock_waic_result1.waic_se = 5.0

    mock_waic_result2 = MagicMock()
    mock_waic_result2.waic = 110.0
    mock_waic_result2.waic_se = 6.0

    mock_waic.side_effect = [mock_waic_result1, mock_waic_result2]

    # Create models dictionary
    models = {
        "model1": mock_model,
        "model2": mock_model,  # Using the same mock model for simplicity
    }

    # Create posterior samples dictionary
    posterior_samples = {
        "model1": mock_posterior_samples,
        "model2": mock_posterior_samples,
    }

    # Compare models
    result = comparison_instance.compare_models(
        models=models,
        posterior_samples=posterior_samples,
        data=sample_data,
        metric="waic",
    )

    # Check result
    assert isinstance(result, ComparisonResult)
    assert result.metric_name == "WAIC"
    assert result.values == {"model1": 100.0, "model2": 110.0}
    assert result.standard_errors == {"model1": 5.0, "model2": 6.0}
    # Skip checking differences as they might not be computed in the mock


@patch("arviz.loo")
@patch.object(BayesianModelComparison, "_extract_log_likelihood")
def test_compare_models_loo(
    mock_extract_log_likelihood,
    mock_loo,
    comparison_instance,
    mock_model,
    mock_posterior_samples,
    sample_data,
):
    """Test compare_models method with LOO."""
    # Mock _extract_log_likelihood to return PyTorch tensors
    mock_log_likes = torch.ones((100, 10))
    mock_extract_log_likelihood.return_value = mock_log_likes

    # Mock arviz.loo return values
    mock_loo_result1 = MagicMock()
    mock_loo_result1.loo = 100.0
    mock_loo_result1.loo_se = 5.0

    mock_loo_result2 = MagicMock()
    mock_loo_result2.loo = 110.0
    mock_loo_result2.loo_se = 6.0

    mock_loo.side_effect = [mock_loo_result1, mock_loo_result2]

    # Create models dictionary
    models = {
        "model1": mock_model,
        "model2": mock_model,  # Using the same mock model for simplicity
    }

    # Create posterior samples dictionary
    posterior_samples = {
        "model1": mock_posterior_samples,
        "model2": mock_posterior_samples,
    }

    # Compare models
    result = comparison_instance.compare_models(
        models=models,
        posterior_samples=posterior_samples,
        data=sample_data,
        metric="loo",
    )

    # Check result
    assert isinstance(result, ComparisonResult)
    assert result.metric_name == "LOO"
    assert result.values == {"model1": 100.0, "model2": 110.0}
    assert result.standard_errors == {"model1": 5.0, "model2": 6.0}
    # Skip checking differences as they might not be computed in the mock


@patch.object(BayesianModelComparison, "_compute_log_marginal_likelihood")
def test_compare_models_bayes_factors(
    mock_compute_lml, comparison_instance, mock_model, sample_data
):
    """Test compare_models_bayes_factors method."""
    # Mock _compute_log_marginal_likelihood return values
    mock_compute_lml.side_effect = [
        -10.0,  # model1
        -12.0,  # model2
        -11.0,  # model3
    ]

    # Create models dictionary
    models = {
        "model1": mock_model,
        "model2": mock_model,
        "model3": mock_model,
    }

    # Compare models
    result = comparison_instance.compare_models_bayes_factors(
        models=models,
        data=sample_data,
        reference_model="model1",
    )

    # Check result
    assert isinstance(result, ComparisonResult)
    assert result.metric_name == "Bayes Factor"
    assert len(result.values) == 3
    # Check specific values
    assert result.values["model1"] == 1.0  # Relative to itself
    assert np.isclose(result.values["model2"], np.exp(-12.0 - (-10.0)), rtol=1e-5)
    assert np.isclose(result.values["model3"], np.exp(-11.0 - (-10.0)), rtol=1e-5)
    # Check metadata
    assert result.metadata == {"reference_model": "model1"}


def test_select_best_model():
    """Test select_best_model function."""
    # Test for information criteria (lower is better)
    waic_values = {"model1": 100.0, "model2": 110.0, "model3": 90.0}

    # Create differences - model3 is 10 units better than model1 and 20 units better than model2
    differences = {
        "model1": {"model2": -10.0, "model3": 10.0},
        "model2": {"model1": 10.0, "model3": 20.0},
        "model3": {"model1": -10.0, "model2": -20.0},
    }

    waic_result = ComparisonResult(
        metric_name="WAIC",
        values=waic_values,
        differences=differences,
    )

    # Threshold = 2, differences are significant
    best_model, is_significant = select_best_model(waic_result, threshold=2.0)
    assert best_model == "model3"
    assert is_significant is True

    # Threshold = 15, differences with model1 are not significant
    best_model, is_significant = select_best_model(waic_result, threshold=15.0)
    assert best_model == "model3"
    assert is_significant is False

    # Test for Bayes factors (higher is better)
    bf_values = {"model1": 3.0, "model2": 1.0, "model3": 2.0}

    # Create differences - model1 is 3x better than model2 and 1.5x better than model3
    differences = {
        "model1": {"model2": np.log(3.0/1.0), "model3": np.log(3.0/2.0)},
        "model2": {"model1": np.log(1.0/3.0), "model3": np.log(1.0/2.0)},
        "model3": {"model1": np.log(2.0/3.0), "model2": np.log(2.0/1.0)},
    }

    bf_result = ComparisonResult(
        metric_name="Bayes Factor",
        values=bf_values,
        differences=differences,
    )

    # Threshold = 2, differences with model3 are significant
    print(f"BF values: {bf_values}")
    print(f"BF differences: {differences}")
    best_model, is_significant = select_best_model(bf_result, threshold=2.0)
    print(f"Best model: {best_model}, is_significant: {is_significant}")
    assert best_model == "model1"
    assert is_significant is True

    # Threshold = 5, no differences are significant
    best_model, is_significant = select_best_model(bf_result, threshold=5.0)
    assert best_model == "model1"
    assert is_significant is False


def test_create_comparison_table():
    """Test create_comparison_table function."""
    # Create comparison results
    waic_values = {"model1": 100.0, "model2": 110.0, "model3": 90.0}
    waic_se = {"model1": 5.0, "model2": 6.0, "model3": 4.0}
    waic_result = ComparisonResult(
        metric_name="WAIC",
        values=waic_values,
        standard_errors=waic_se,
    )

    loo_values = {"model1": 105.0, "model2": 115.0, "model3": 95.0}
    loo_se = {"model1": 5.5, "model2": 6.5, "model3": 4.5}
    loo_result = ComparisonResult(
        metric_name="LOO",
        values=loo_values,
        standard_errors=loo_se,
    )

    # Create comparison table
    table = create_comparison_table([waic_result, loo_result])

    # Check table
    assert isinstance(table, pd.DataFrame)
    assert list(table.columns) == ["model", "WAIC", "WAIC_se", "LOO", "LOO_se"]
    assert len(table) == 3
    assert set(table["model"]) == {"model1", "model2", "model3"}

    # Check values
    model1_row = table[table["model"] == "model1"].iloc[0]
    assert model1_row["WAIC"] == 100.0
    assert model1_row["WAIC_se"] == 5.0
    assert model1_row["LOO"] == 105.0
    assert model1_row["LOO_se"] == 5.5


@patch.object(BayesianModelComparison, "_extract_log_likelihood")
def test_extract_log_likelihood(
    mock_extract_log_likelihood,
    comparison_instance,
    mock_model,
    mock_posterior_samples,
    sample_data,
):
    """Test _extract_log_likelihood method."""
    # Mock the patched method to return a known tensor
    expected_log_likes = torch.ones((100, 10)) * 2.0
    mock_extract_log_likelihood.return_value = expected_log_likes

    # Call the method - this will use our mocked version
    log_likes = BayesianModelComparison._extract_log_likelihood(
        mock_model,
        mock_posterior_samples,
        sample_data,
    )

    # Check the result
    assert torch.all(log_likes == expected_log_likes)


def test_extract_log_likelihood_missing_observations(
    comparison_instance, mock_model, mock_posterior_samples
):
    """Test _extract_log_likelihood method when observations are missing."""
    # Create data without observations
    data = {"x": torch.ones((10, 5))}

    # Expect a ValueError
    with pytest.raises(ValueError, match="Data dictionary must contain 'observations' key"):
        BayesianModelComparison._extract_log_likelihood(
            mock_model,
            mock_posterior_samples,
            data,
        )


@patch("pyro.infer.Importance")
def test_compute_log_marginal_likelihood(
    mock_importance, comparison_instance, mock_model, sample_data
):
    """Test _compute_log_marginal_likelihood method."""
    # Mock pyro.infer.Importance.run() return value
    mock_importance_instance = MagicMock()
    mock_importance.return_value = mock_importance_instance

    # Mock importance_results.log_mean() return value
    mock_importance_results = MagicMock()
    mock_importance_results.log_mean.return_value = torch.tensor(-10.0)
    mock_importance_instance.run.return_value = mock_importance_results

    # Call the method
    log_ml = comparison_instance._compute_log_marginal_likelihood(
        model=mock_model,
        data=sample_data,
        num_samples=1000,
    )

    # Check the result
    assert log_ml == -10.0

    # Check that Importance was called
    mock_importance.assert_called_once()
    # Check that importance_instance.run() was called
    mock_importance_instance.run.assert_called_once()
