---
title: Testing Framework and Reference
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
---

# Introduction {#sec-introduction}

## Purpose of this Document {#sec-purpose}

This document describes the testing approach and infrastructure for the `pyrovelocity` library. It serves as both a reference for the existing test suite and a guide for developing new tests. The document establishes traceability between tests and the AMDiRE artifacts ([Context Specification](context.qmd), [Requirements Specification](requirements.qmd), and [System Specification](architecture.qmd)) to ensure comprehensive test coverage of all system aspects.

## Document Conventions {#sec-conventions}

This document uses the following conventions:

- *Italics* are used for terms defined in the glossary
- **Bold** is used for emphasis and key concepts
- Blue text indicates a cross-reference to another section
- Test IDs follow the format TEST-[type]-[number] (e.g., TEST-UNIT-01, TEST-BDD-01)
- AMDiRE element references use their original identifiers (e.g., REQ-01, SC-01, COMP-01)

## References {#sec-references}

1. [Scientific Context Specification](context.qmd)
2. [Scientific Requirements Specification](requirements.qmd)
3. [Scientific System Specification](architecture.qmd)
4. [PyroVelocity Validation Framework (PRD-678-VALID)](https://github.com/YosefLab/pyrovelocity/tree/main/pyrovelocity-planning/prds/prd-678-valid)
5. [pytest-bdd Documentation](https://pytest-bdd.readthedocs.io/)

## Document Overview {#sec-overview}

This Testing Framework and Reference Documentation is organized to provide a comprehensive view of the testing strategy and implementation. It begins with the testing philosophy and infrastructure, then details the test development workflow and execution procedures. The AMDiRE traceability sections establish connections between tests and the elements defined in the Context, Requirements, and System specifications. Reference tables track test coverage and status, while subsequent sections provide detailed guidance on BDD feature files, step definitions, and advanced testing topics.

# Testing Philosophy {#sec-testing-philosophy}

Our testing approach implements a dual strategy that addresses both implementation details and user-facing behavior:

1. **Unit and Integration Tests** (`tests/models/modular/*.py`):
   - Traditional pytest-based tests focusing on implementation details
   - Verify internal functions, edge cases, and error handling
   - Target high code coverage and technical correctness
   - Validate that individual components work as expected in isolation

2. **Behavior-Driven Tests**:
   - **Feature Definitions** (`tests/features/models/modular/*.feature`):
     - User-centric specifications written in Gherkin syntax
     - Focus on requirements and use cases from the user's perspective
     - Serve as living documentation of expected behavior
   - **Feature Implementations** (`tests/models/modular/integration/*.py`):
     - Connect Gherkin specifications to actual code
     - Test how components work together to deliver user-facing functionality
     - Validate that the system behaves according to specifications

This approach follows Acceptance Test-Driven Development (ATDD), where feature files are written before implementation to document requirements and guide development.

### Key Principles {#sec-key-principles}

PyroVelocity's testing philosophy emphasizes:

- **Protocol-First Testing**: Testing against interfaces rather than concrete implementations
- **Real Objects Over Mocks**: Creating simple instances of real objects rather than using mocking tools
- **Test-Driven Development**: Writing tests alongside code development
- **Reasonable Constraints**: Ensuring tests provide reasonable constraints on the code
- **Systematic Validation**: Comprehensive validation of model implementations
- **AMDiRE Traceability**: Connecting tests to context, requirements, and system elements

# Directory Structure {#sec-directory-structure}

The tests are organized to mirror the source code structure while separating unit tests from BDD feature definitions and step implementations:

```
src/pyrovelocity/tests/
├── __init__.py
├── conftest.py                # Shared pytest fixtures and configuration
├── fixtures/                  # Test data generation utilities
├── models/                    # Tests for model components
│   ├── __init__.py
│   ├── modular/               # Tests for modular implementation
│   │   ├── __init__.py
│   │   ├── conftest.py        # Modular-specific fixtures
│   │   ├── components/        # Unit tests for component implementations
│   │   │   ├── test_dynamics.py
│   │   │   ├── test_guides.py
│   │   │   ├── test_likelihoods.py
│   │   │   ├── test_observations.py
│   │   │   └── test_priors.py
│   │   ├── integration/       # BDD step implementations
│   │   │   ├── __init__.py
│   │   │   ├── conftest.py    # BDD-specific fixtures
│   │   │   ├── test_component_integration.py
│   │   │   ├── test_dynamics_model_steps.py
│   │   │   ├── test_guide_model_steps.py
│   │   │   ├── test_likelihood_model_steps.py
│   │   │   ├── test_model_steps.py
│   │   │   ├── test_observation_model_steps.py
│   │   │   └── test_prior_model_steps.py
│   │   ├── selection/         # Tests for model selection
│   │   ├── utils/             # Tests for utility modules
│   │   ├── test_anndata_integration.py
│   │   ├── test_comparison.py
│   │   ├── test_factory.py
│   │   ├── test_interfaces.py
│   │   ├── test_model.py
│   │   ├── test_registry.py
│   │   └── test_unified_config.py
│   ├── jax/                   # Tests for JAX implementation (not yet documented)
│   └── test_*.py              # Tests for legacy implementation
├── features/                  # BDD feature specifications
│   └── models/
│       └── modular/
│           ├── dynamics_model.feature
│           ├── guide_model.feature
│           ├── likelihood_model.feature
│           ├── model.feature
│           ├── observation_model.feature
│           └── prior_model.feature
├── plots/                     # Tests for plotting functions
├── utils/                     # Tests for utility functions
├── validation/                # Tests for validation framework
└── test_*.py                  # Package-level tests
```

# Test Development Workflow {#sec-test-development-workflow}

Our recommended workflow follows these steps:

1. **Review AMDiRE Artifacts**: Examine the Context, Requirements, and System specifications to understand what needs to be tested
2. **Identify Test Cases**: Derive test cases from AMDiRE elements, ensuring traceability to specific requirements and system components
3. **Specify Behavior**: Create or update a feature file in `features/` for new functionality, referencing relevant AMDiRE elements
4. **Implement Step Definitions**: Create/update step definitions in the subpackage's `integration/` directory
5. **Add Unit Tests**: Create detailed unit tests in the subpackage's test modules
6. **Implement Source Code**: Develop the actual functionality in the source code
7. **Update Traceability**: Update the traceability matrices in this document to reflect the new tests
8. **Update Documentation**: Update the reference tables in this document

# Running Tests {#sec-running-tests}

### All Tests

```bash
source .venv/bin/activate
pytest src/pyrovelocity/tests
```

### Only Modular Implementation Tests

```bash
source .venv/bin/activate
pytest src/pyrovelocity/tests/models/modular
```

### Only BDD Tests

```bash
source .venv/bin/activate
pytest src/pyrovelocity/tests/models/modular/integration
```

### Specific Component Tests

```bash
source .venv/bin/activate
pytest src/pyrovelocity/tests/models/modular/components/test_dynamics.py
```

### Running with Coverage

```bash
source .venv/bin/activate
pytest src/pyrovelocity/tests --cov=pyrovelocity --cov-report=term-missing
```

### Profiling and Benchmarking

PyroVelocity uses Scalene for profiling and benchmarking:

```bash
source .venv/bin/activate
scalene --profile-all src/pyrovelocity/tests/models/test_model.py
```

# AMDiRE Traceability {#sec-amdire-traceability}

This section establishes traceability between tests and the elements defined in the AMDiRE artifacts (Context Specification, Requirements Specification, and System Specification). This traceability ensures that all aspects of the system are adequately tested and that the tests align with the project's objectives, requirements, and design.

## Context Element Traceability {#sec-context-element-traceability}

This table maps tests to elements in the Context Specification, ensuring that tests validate the scientific objectives, domain model elements, and constraints defined in the project context.

| Test ID | Test Type | Test Name | Context Element ID | Context Element Type | Validation Approach |
|---------|-----------|-----------|-------------------|---------------------|---------------------|
| TEST-BDD-01 | BDD | Standard dynamics model | RG-01 | Research Goal | Verifies that the dynamics model correctly implements RNA velocity equations |
| TEST-BDD-02 | BDD | Uncertainty quantification | RG-02 | Research Goal | Validates uncertainty estimates in velocity computation |
| TEST-UNIT-01 | Unit | Parameter inference | RG-03 | Research Goal | Tests parameter inference from synthetic data |
| TEST-BDD-03 | BDD | Library size correction | SC-01 | Scientific Constraint | Ensures model respects biological constraints |

## Requirements Traceability {#sec-requirements-traceability}

This table maps tests to elements in the Requirements Specification, ensuring that tests validate the functional requirements, quality requirements, and computational models defined for the system.

| Test ID | Test Type | Test Name | Requirement ID | Requirement Type | Coverage Level |
|---------|-----------|-----------|---------------|-----------------|---------------|
| TEST-BDD-04 | BDD | Model training | FN-03 | Functional Requirement | Complete |
| TEST-UNIT-02 | Unit | Posterior sampling | FN-04 | Functional Requirement | Complete |
| TEST-BDD-05 | BDD | Velocity computation | FN-05 | Functional Requirement | Complete |
| TEST-UNIT-03 | Unit | AnnData integration | FN-08 | Functional Requirement | Partial |
| TEST-UNIT-04 | Unit | Performance benchmark | PERF-01 | Performance Requirement | Complete |

## System Element Traceability {#sec-system-element-traceability}

This table maps tests to elements in the System Specification, ensuring that tests validate the components, interfaces, and algorithms defined in the system design.

| Test ID | Test Type | Test Name | System Element ID | Element Type | Test Scope |
|---------|-----------|-----------|------------------|-------------|------------|
| TEST-BDD-06 | BDD | Component integration | CMP-01 | Component | Interface verification |
| TEST-UNIT-05 | Unit | Dynamics model | CMP-02 | Component | Functional correctness |
| TEST-UNIT-06 | Unit | Prior model | CMP-03 | Component | Functional correctness |
| TEST-UNIT-07 | Unit | Likelihood model | CMP-04 | Component | Functional correctness |
| TEST-UNIT-08 | Unit | RNA velocity algorithm | MM-01 | Mathematical Model | Correctness and stability |

## Traceability Matrix {#sec-traceability-matrix}

This comprehensive matrix provides a cross-reference between tests and all AMDiRE elements, helping to identify gaps in test coverage and ensure that all aspects of the system are adequately tested.

| AMDiRE Element ID | Element Type | Element Description | Test IDs | Coverage Status |
|-------------------|--------------|---------------------|----------|----------------|
| RG-01 | Research Goal | Probabilistic RNA Velocity | TEST-BDD-01, TEST-UNIT-05 | Complete |
| RG-02 | Research Goal | Uncertainty Quantification | TEST-BDD-02 | Partial |
| RG-03 | Research Goal | Parameter Inference | TEST-UNIT-01, TEST-BDD-07 | Planned |
| FN-03 | Functional Requirement | Model Training | TEST-BDD-04 | Complete |
| FN-05 | Functional Requirement | Velocity Computation | TEST-BDD-05 | Complete |
| CAP-06 | Capability | Parameter Recovery Validation | TEST-BDD-07, TEST-UNIT-09 | Planned |
| CAP-07 | Capability | Model Comparison | TEST-BDD-08, TEST-UNIT-10 | Planned |
| CMP-01 | Component | PyroVelocityModel | TEST-BDD-06 | Complete |
| CMP-02 | Component | DynamicsModel | TEST-UNIT-05, TEST-BDD-01 | Complete |
| SC-03 | Scientific Constraint | Parameter Identifiability | TEST-BDD-07 | Planned |
| MM-01 | Mathematical Model | RNA Velocity Dynamics | TEST-UNIT-08 | Complete |
| ALG-02 | Algorithm | Parameter Recovery Validation | TEST-UNIT-09 | Planned |

# Test Coverage Reference {#sec-test-coverage-reference}

## Feature Specifications Reference {#sec-feature-specifications-reference}

The table below tracks all feature specifications defined in our BDD feature files for the modular implementation. This serves as a living document of the intended functionality for the library.

| Module | Feature | Status | Description | Related AMDiRE Elements |
|--------|---------|--------|-------------|------------------------|
| dynamics_model | Standard dynamics model computes expected counts | ✅ Implemented | Verify that the StandardDynamicsModel correctly computes expected unspliced and spliced counts | MM-01, CMP-02, FN-05 |
| dynamics_model | Standard dynamics model computes steady state | ✅ Implemented | Verify that the StandardDynamicsModel correctly computes steady state values | MM-01, CMP-02 |
| dynamics_model | Legacy dynamics model matches legacy implementation | ✅ Implemented | Verify that the LegacyDynamicsModel matches the legacy implementation | CMP-02, TC-04 |
| dynamics_model | Dynamics model handles edge cases | ✅ Implemented | Verify that the dynamics model handles edge cases gracefully | CMP-02, REL-02 |
| dynamics_model | Dynamics model with library size correction | ✅ Implemented | Verify that the dynamics model correctly applies library size correction | CMP-02, SC-01 |
| prior_model | LogNormal prior model creates stochastic nodes | ✅ Implemented | Verify that the LogNormalPriorModel creates appropriate stochastic nodes | CMP-03, ALG-02 |
| likelihood_model | Poisson likelihood model defines distributions | ✅ Implemented | Verify that the PoissonLikelihoodModel defines appropriate Poisson distributions | CMP-04, ALG-02 |
| model | PyroVelocity model integrates components | ✅ Implemented | Verify that the PyroVelocityModel correctly integrates all components | CMP-01, SG-01 |
| model | PyroVelocity model trains on AnnData | ✅ Implemented | Verify that the PyroVelocityModel can train on AnnData objects | CMP-01, FN-03, INT-01 |
| validation | Parameter recovery validation | 🔄 Planned | Verify that the model can recover known parameters from synthetic data | CAP-06, RG-03, SC-03, ALG-02 |
| validation | Model comparison | 🔄 Planned | Verify that different model implementations can be compared | CAP-07, TC-04, SG-03 |

### Legend
- ✅ Implemented: Feature is fully implemented and tested
- ⚠️ In Progress: Implementation or testing is underway
- 📝 Specified: Feature is specified but not yet implemented
- 🔄 Planned: Feature is planned but not fully specified

## Unit Test Coverage Reference {#sec-unit-test-coverage-reference}

This table tracks the unit test coverage for each module in the modular implementation, helping identify areas that need additional testing.

| Module | Test File | Test Coverage | Test Cases | Missing Coverage | Related AMDiRE Elements |
|--------|-----------|---------------|------------|------------------|------------------------|
| components/dynamics.py | test_dynamics.py | High | 10+ | Edge cases with β = γ | CMP-02, MM-01 |
| components/guides.py | test_guides.py | High | 5+ | Custom initialization | CMP-06, ALG-02 |
| components/likelihoods.py | test_likelihoods.py | High | 5+ | Zero count handling | CMP-04, ALG-02 |
| components/observations.py | test_observations.py | High | 5+ | Missing value handling | CMP-05 |
| components/priors.py | test_priors.py | High | 5+ | None | CMP-03, ALG-02 |
| model.py | test_model.py | High | 10+ | None | CMP-01, FN-03, FN-05 |
| factory.py | test_factory.py | High | 10+ | None | AD-05 |
| registry.py | test_registry.py | High | 5+ | None | AD-04 |

# BDD Feature Files Explained {#sec-bdd-feature-files-explained}

Our feature files use Gherkin syntax to describe expected behavior. Each feature file follows this structure:

```gherkin
Feature: [Feature Name]
  As a [type of user]
  I want to [perform some action]
  So that [achieve some benefit]

  # AMDiRE Traceability
  # Context: [Context Element IDs]
  # Requirements: [Requirement IDs]
  # System: [System Element IDs]

  Background:
    Given [common setup steps]

  Scenario: [Specific use case name]
    Given [preconditions]
    When [actions]
    Then [expected results]
```

### Example Feature Files

PyroVelocity uses feature files to describe the expected behavior of each component. Here's an example from the dynamics model:

```gherkin
Feature: Dynamics Model
  As a computational biologist
  I want to model RNA velocity dynamics
  So that I can understand transcriptional dynamics in single-cell data

  # AMDiRE Traceability
  # Context: RG-01 (Probabilistic RNA Velocity)
  # Requirements: FN-05 (Velocity Computation)
  # System: CMP-02 (DynamicsModel), MM-01 (RNA Velocity Dynamics)

  Background:
    Given I have a dynamics model component
    And I have input data with unspliced and spliced counts

  Scenario Outline: Standard dynamics model computes expected counts
    Given I have a StandardDynamicsModel
    When I run the forward method with alpha <alpha>, beta <beta>, and gamma <gamma>
    Then the model should compute expected unspliced and spliced counts
    And the expected counts should follow RNA velocity dynamics

    Examples:
      | alpha | beta | gamma |
      | 1.0   | 0.5  | 0.2   |
```

### Deriving Tests from AMDiRE Artifacts

When creating BDD feature files, we follow these guidelines to ensure alignment with AMDiRE artifacts:

1. **From Context Specification**:
   - Derive test scenarios from research goals (RG-xx)
   - Ensure tests respect scientific constraints (SC-xx)
   - Include validation of domain model elements

2. **From Requirements Specification**:
   - Create test scenarios for each functional requirement (FN-xx)
   - Develop performance tests for performance requirements (PERF-xx)
   - Include reliability tests for reliability requirements (REL-xx)

3. **From System Specification**:
   - Test each component's functionality (CMP-xx)
   - Validate algorithm implementations (ALG-xx)
   - Test interfaces between components

# Step Definitions Structure {#sec-step-definitions-structure}

Step definition files connect feature files to actual test code. They are organized by component in the `integration/` directory:

```python
# src/pyrovelocity/tests/models/modular/integration/test_dynamics_model_steps.py
from importlib.resources import files
import pytest
import torch
from pytest_bdd import scenarios, given, when, then, parsers

# Link to the feature file using importlib.resources
scenarios(str(files("pyrovelocity.tests.features") / "models" / "modular" / "dynamics_model.feature"))

# Import the components
from pyrovelocity.models.modular.components import (
    LegacyDynamicsModel,
    StandardDynamicsModel,
)

@given("I have a StandardDynamicsModel", target_fixture="standard_dynamics_model")
def standard_dynamics_model_fixture(bdd_standard_dynamics_model):
    """Get a StandardDynamicsModel from the fixture."""
    return bdd_standard_dynamics_model

@when(parsers.parse("I run the forward method with alpha {alpha}, beta {beta}, and gamma {gamma}"), target_fixture="run_forward_method_with_parameters")
def run_forward_method_with_parameters_fixture(standard_dynamics_model, input_data, alpha, beta, gamma):
    """Run the forward method with the given parameters."""
    # Convert string parameters to float
    alpha_val = float(alpha)
    beta_val = float(beta)
    gamma_val = float(gamma)

    # Create context with input data and parameters
    context = {
        "u_obs": input_data["u_obs"],
        "s_obs": input_data["s_obs"],
        "alpha": torch.tensor([alpha_val] * input_data["n_genes"]),
        "beta": torch.tensor([beta_val] * input_data["n_genes"]),
        "gamma": torch.tensor([gamma_val] * input_data["n_genes"]),
    }

    # Run the forward method
    result_context = standard_dynamics_model.forward(context)

    # Store the result for later steps
    return result_context
```

# Validation Framework {#sec-validation-framework}

PyroVelocity includes a validation framework (PRD-678-VALID) to verify that the modular model can replicate the legacy model and to validate the model's ability to recover known parameters from synthetic data. This framework is a critical part of our testing strategy, ensuring that the refactored implementation maintains compatibility with existing workflows and that the model is internally consistent.

## Key Validation Components

- **Direct Comparison**: Comparing outputs between legacy and modular implementations
- **Synthetic Data Validation**: Using synthetic data with known properties
- **Real Data Validation**: Using downsampled real datasets (e.g., `tests/data/preprocessed_pancreas_50_7.json`)
- **Parameter Validation**: Ensuring parameter inference matches between implementations
- **Shape Validation**: Verifying tensor shapes are consistent between implementations
- **Parameter Recovery Validation**: Validating model's ability to recover known parameters from synthetic data

## Implementation

The validation framework is implemented in the `src/pyrovelocity/tests/validation/` directory and includes:

- Direct comparison tests for model outputs
- Metrics for quantifying differences between implementations
- Visualization tools for analyzing differences
- Utilities for creating patched versions of components to address compatibility issues
- Parameter recovery validation tools for generating synthetic data and evaluating parameter recovery

## Parameter Recovery Validation

Parameter recovery validation is a fundamental technique in probabilistic modeling that assesses whether a model's inference procedure can correctly recover known parameter values from synthetic data generated by the model itself. It serves as a crucial sanity check before applying models to real-world data where true parameter values are unknown.

The parameter recovery validation workflow follows these steps:

1. **Parameter Generation**: Sample parameter sets from prior distributions
2. **Synthetic Data Generation**: Create synthetic data using known parameters
3. **Inference**: Perform inference on synthetic data to estimate parameters
4. **Evaluation**: Compare estimated parameters to original known values
5. **Metrics Computation**: Calculate recovery accuracy, precision, and coverage metrics

The validation framework provides tools for each step of this workflow, including:

- Functions for generating parameter sets from prior distributions
- Functions for creating synthetic data with known parameters
- Functions for performing inference on synthetic data
- Functions for comparing estimated parameters to true values
- Functions for calculating and visualizing recovery metrics

## AMDiRE Traceability

The validation framework addresses several key AMDiRE elements:

- **TC-04**: Backward Compatibility constraint
- **REQ-01**: Parameter Estimation Accuracy requirement
- **SG-03**: Comprehensive Testing system goal
- **REL-01**: Training Convergence reliability requirement
- **CAP-06**: Parameter Recovery Validation capability
- **CAP-07**: Model Comparison capability
- **RG-03**: Parameter Inference research goal
- **SC-03**: Parameter Identifiability scientific constraint

# Contributing New Tests {#sec-contributing-new-tests}

When contributing new tests to the project:

1. **Review AMDiRE Artifacts**: Examine the Context, Requirements, and System specifications
2. **Check Existing Coverage**: Review the traceability matrices and test reference tables
3. **Identify Coverage Gaps**: Look for AMDiRE elements with incomplete test coverage
4. **Update Feature Files**: If implementing a new feature, add or update the relevant feature file with AMDiRE traceability comments
5. **Follow the Workflow**: Implement step definitions and unit tests as outlined above
6. **Establish Traceability**: Ensure all tests are linked to specific AMDiRE elements
7. **Run Tests Locally**: Ensure all tests pass before submitting a PR
8. **Update Documentation**: Update the traceability matrices and reference tables in this document

## Test Coverage Guidelines

To ensure comprehensive test coverage across all AMDiRE elements:

1. **Context Elements**:
   - Every research goal (RG-xx) should have at least one associated test
   - All scientific constraints (SC-xx) should be validated by appropriate tests
   - Domain model elements should have tests verifying their implementation

2. **Requirements Elements**:
   - Each functional requirement (FN-xx) should have at least one BDD test
   - Performance requirements (PERF-xx) should have benchmarking tests
   - Reliability requirements (REL-xx) should have error handling and edge case tests

3. **System Elements**:
   - Every component (CMP-xx) should have unit tests for its internal functionality
   - All algorithms (ALG-xx) should have tests for correctness and edge cases
   - Interfaces (INT-xx) should have integration tests

# Fixtures and Testing Utilities {#sec-fixtures-and-testing-utilities}

PyroVelocity provides a comprehensive set of fixtures for testing, defined in various `conftest.py` files:

## Main Fixtures (`src/pyrovelocity/tests/conftest.py`)

| Fixture Name | Purpose | Scope |
|--------------|---------|-------|
| `adata_preprocessed_pancreas_50_7` | Provides preprocessed pancreas dataset | Function |
| `adata_trained_pancreas_50_7` | Provides trained pancreas dataset | Function |
| `default_sample_data` | Provides synthetic sample data | Function |
| `temp_file_path` | Provides a temporary file path | Function |

## Modular Implementation Fixtures (`src/pyrovelocity/tests/models/modular/conftest.py`)

These fixtures are specific to the modular implementation and provide test instances of various components.

## BDD-Specific Fixtures (`src/pyrovelocity/tests/models/modular/integration/conftest.py`)

| Fixture Name | Purpose | Scope |
|--------------|---------|-------|
| `bdd_simple_data` | Provides simple data for BDD testing | Function |
| `bdd_model_parameters` | Provides model parameters for BDD testing | Function |
| `bdd_standard_dynamics_model` | Provides a StandardDynamicsModel | Function |
| `bdd_legacy_dynamics_model` | Provides a LegacyDynamicsModel | Function |
| `bdd_lognormal_prior_model` | Provides a LogNormalPriorModel | Function |
| `bdd_pyro_velocity_model` | Provides a PyroVelocityModel | Function |
| `clear_pyro_param_store` | Clears Pyro's parameter store | Function (autouse) |

# Advanced Testing Topics {#sec-advanced-testing-topics}

## Parameterized Testing

Both unit tests and BDD scenarios can be parameterized:

```python
# Unit test parameterization
@pytest.mark.parametrize("alpha,beta,gamma,expected_u_ss,expected_s_ss", [
    (1.0, 0.5, 0.2, 2.0, 5.0),
    (2.0, 1.0, 0.5, 2.0, 4.0),
])
def test_steady_state(alpha, beta, gamma, expected_u_ss, expected_s_ss):
    model = StandardDynamicsModel()
    u_ss, s_ss = model.steady_state(
        torch.tensor([alpha]),
        torch.tensor([beta]),
        torch.tensor([gamma]),
    )
    assert torch.allclose(u_ss, torch.tensor([expected_u_ss]))
    assert torch.allclose(s_ss, torch.tensor([expected_s_ss]))
```

```gherkin
# BDD parameterization with Scenario Outline
Scenario Outline: Standard dynamics model computes steady state
  Given I have a StandardDynamicsModel
  When I compute the steady state with alpha <alpha>, beta <beta>, and gamma <gamma>
  Then the steady state unspliced should equal alpha/beta
  And the steady state spliced should equal alpha/gamma

  Examples:
    | alpha | beta | gamma |
    | 1.0   | 0.5  | 0.2   |
    | 2.0   | 1.0  | 0.5   |
```

## Testing with Pyro

PyroVelocity provides utilities for testing with Pyro:

```python
def test_pyro_model(clear_pyro_param_store):
    """Test a Pyro model."""
    # Define a simple model
    def model():
        x = pyro.sample("x", pyro.distributions.Normal(0, 1))
        return x

    # Define a guide
    def guide():
        loc = pyro.param("loc", torch.tensor(0.0))
        scale = pyro.param("scale", torch.tensor(1.0), constraint=pyro.constraints.positive)
        pyro.sample("x", pyro.distributions.Normal(loc, scale))

    # Run inference
    pyro.clear_param_store()
    svi = pyro.infer.SVI(
        model=model,
        guide=guide,
        optim=pyro.optim.Adam({"lr": 0.01}),
        loss=pyro.infer.Trace_ELBO(),
    )

    # Train for a few steps
    for _ in range(10):
        loss = svi.step()

    # Check that parameters were updated
    assert pyro.param("loc").item() != 0.0
```

# Continuous Integration {#sec-continuous-integration}

Tests are automatically run as part of the CI/CD pipeline defined in `.github/workflows/cid.yaml`. The CI pipeline includes:

1. **Unit Tests**: Run all unit tests with coverage reporting
2. **BDD Tests**: Run all BDD tests to verify behavior
3. **Coverage Enforcement**: Ensure test coverage meets minimum thresholds

# Glossary {#sec-glossary}

## Testing Terminology

| Term | Definition | Related AMDiRE Elements |
|------|------------|------------------------|
| Unit Test | A test that verifies the functionality of a single function, method, or class in isolation | System components (CMP-xx), algorithms (ALG-xx) |
| Integration Test | A test that verifies the interaction between multiple components | System interfaces (INT-xx) |
| BDD Test | A test written in Gherkin syntax that describes behavior from a user perspective | Functional requirements (FN-xx) |
| Test Fixture | A fixed state of the system used as a baseline for running tests | Data objects (DO-xx) |
| Test Coverage | The percentage of code that is executed during tests | Quality requirements (QR-xx) |
| Protocol-First Testing | Testing against Protocol interfaces rather than concrete implementations | AD-01 (Protocol-First Architecture) |
