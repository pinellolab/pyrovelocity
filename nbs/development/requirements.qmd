---
title: "Requirements Specification"
format:
  html:
    mermaid-format: js
    mermaid:
      theme: dark
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
bibliography: ../references.bib
csl: ../bibstyle.csl
---

# Introduction {#sec-introduction}

## Purpose of this Document {#sec-purpose}

This Requirements Specification defines what PyroVelocity should do from an external (black-box) perspective without specifying how it will be implemented internally. This document builds upon the [Scientific Context Specification](context.qmd) by transforming research needs into specific system requirements. It serves as the primary reference for domain scientists, computational specialists, developers, testers, and stakeholders to ensure a shared understanding of the system's expected behavior conceptually prior to detailed design considerations.

## Document Conventions {#sec-conventions}

This document uses the following conventions:

- *Italics* are used for terms defined in the glossary
- **Bold** is used for emphasis and key concepts
- Blue text indicates a cross-reference to another section
- REQ-XXX identifiers are used for individually traceable requirements
- Mathematical formulas are presented using LaTeX notation
- Code examples are presented in monospaced font
- Computational complexity is expressed using Big O notation

## References {#sec-references}

1. [Scientific Context Specification](context.qmd)
2. [System Architecture and Design](architecture.qmd)
3. La Manno, G., Soldatov, R., Zeisel, A. et al. RNA velocity of single cells. Nature 560, 494–498 (2018). https://doi.org/10.1038/s41586-018-0414-6
4. Bergen, V., Lange, M., Peidli, S. et al. Generalizing RNA velocity to transient cell states through dynamical modeling. Nat Biotechnol 38, 1408–1414 (2020). https://doi.org/10.1038/s41587-020-0591-3
5. Pyro: Deep Universal Probabilistic Programming. https://pyro.ai/
6. AnnData: Annotated Data. https://anndata.readthedocs.io/

## Document Overview {#sec-overview}

This Requirements Specification is organized to progressively define the PyroVelocity system from an external perspective. It begins with a system vision that outlines the overall scope and context, followed by detailed models of usage, computation, and data. The functional requirements organize system capabilities, while quality requirements define non-functional aspects with particular attention to scientific computing needs like accuracy, performance, and reproducibility. System constraints, deployment requirements, and development requirements outline limitations and implementation considerations. The risk assessment identifies potential issues, and the glossary ensures consistent terminology.

# System Vision {#sec-system-vision}

## System Overview {#sec-system-overview}

PyroVelocity is a probabilistic framework for RNA velocity analysis in single-cell genomics. It provides a Bayesian approach to modeling RNA dynamics, enabling robust uncertainty quantification, parameter inference, and model comparison from single-cell RNA sequencing data. The system implements a modular architecture with interchangeable components for dynamics models, prior distributions, likelihood models, observation models, and inference guides.

PyroVelocity aims to overcome limitations of deterministic RNA velocity methods by:

1. Directly modeling raw spliced and unspliced read counts
2. Providing uncertainty quantification for velocity estimates
3. Synchronizing cell time estimation across genes
4. Enabling multivariate modeling of gene expression
5. Offering diagnostic analyses and visualizations of model uncertainty
6. Supporting model validation, comparison, and selection
7. Integrating with existing single-cell analysis ecosystems

The system is currently transitioning from a legacy implementation to a modular PyTorch/Pyro implementation, with a future migration to a JAX/NumPyro implementation for improved performance and support for multiple inference algorithms.

## System Context {#sec-system-context}

PyroVelocity interfaces with the following external entities:

- **Single-cell RNA-seq Data**: Source data containing spliced and unspliced counts
- **AnnData Objects**: Primary data structure for storing and manipulating single-cell data
- **Scanpy/scVelo**: Ecosystem tools for preprocessing and visualization
- **Pyro/PyTorch**: Core probabilistic programming and tensor computation frameworks
- **JAX/NumPyro**: Alternative backend for improved performance and inference algorithms
- **torchode/diffrax**: Differential equation solvers for dynamical simulation
- **beartype/jaxtyping**: Runtime type checking and array shape annotations
- **Flyte/hydra-zen**: Workflow orchestration and configuration management

```{mermaid}
graph TB
    User["Data Scientist / Researcher"] -->|uses| Library["PyroVelocity"]
    Library -->|depends on| PyTorch["PyTorch"]
    Library -->|depends on| Pyro["Pyro"]
    Library -->|depends on| JAX["JAX"]
    Library -->|depends on| NumPyro["NumPyro"]
    Library -->|depends on| Torchode["torchode"]
    Library -->|depends on| Diffrax["diffrax"]
    Library -->|depends on| Beartype["beartype"]
    Library -->|depends on| Jaxtyping["jaxtyping"]
    Library -->|depends on| Flyte["Flyte"]
    Library -->|depends on| HydraZen["hydra-zen"]
    Library -->|processes| AnnData["AnnData Objects"]
    Library -->|produces| Velocity["RNA Velocity"]
    Library -->|produces| ModelComparison["Model Comparison"]
    Library -->|integrates with| Scanpy["Scanpy"]
    Library -->|integrates with| Scvelo["scVelo"]

    class User user;
    class Library library;
    class PyTorch,Pyro,JAX,NumPyro,Torchode,Diffrax,Beartype,Jaxtyping,Flyte,HydraZen,Scanpy,Scvelo external;
    class AnnData,Velocity,ModelComparison data;
```

## Capability Overview {#sec-capability-overview}

| Capability ID | Capability Name | Description | Priority |
|--------------|-----------------|-------------|----------|
| CAP-01 | Probabilistic RNA Velocity | Bayesian modeling of RNA velocity with uncertainty quantification | High |
| CAP-02 | Parameter Inference | Inference of biologically meaningful parameters from scRNA-seq data using multiple inference algorithms | High |
| CAP-03 | Uncertainty Visualization | Visualization of uncertainty in velocity estimates, parameters, and trajectories | High |
| CAP-04 | AnnData Integration | Seamless integration with AnnData objects and the scanpy ecosystem | High |
| CAP-05 | Modular Architecture | Flexible, component-based architecture with interchangeable parts for systematic model comparison | High |
| CAP-06 | Parameter Recovery Validation | Validation of model's ability to recover known parameters from synthetic data to ensure empirically identifiable models | High |
| CAP-07 | Model Comparison | Systematic comparison of different model architectures, implementations, and inference algorithms | High |
| CAP-08 | JAX/NumPyro Backend | High-performance implementation using JAX/NumPyro with support for MCMC inference | Medium |
| CAP-09 | Workflow Integration | Integration with workflow systems for reproducible analysis and model validation | Medium |
| CAP-10 | Dynamical Simulation | Support for numerical integration of ODEs for non-linear and stochastic dynamics | Medium |

## Usage Scenario Overview {#sec-usage-scenario-overview}

The system supports the following high-level usage scenarios:

- **Data Preprocessing**: Scientists prepare single-cell data for velocity analysis
- **Model Configuration**: Scientists configure model components and parameters
- **Parameter Recovery Validation**: Scientists validate model's ability to recover known parameters from synthetic data
- **Model Training**: Scientists train the model using variational inference or MCMC
- **Posterior Analysis**: Scientists analyze posterior distributions of parameters
- **Velocity Computation**: Scientists compute RNA velocity with uncertainty
- **Result Visualization**: Scientists visualize velocity, parameters, and associated uncertainty
- **Model Comparison**: Scientists systematically compare different model architectures, implementations, and inference algorithms
- **Model Selection**: Scientists select the most appropriate model based on validation metrics
- **Integration with Workflows**: Scientists integrate velocity analysis and model validation into larger pipelines

```{mermaid}
graph TD
    A[Data Preprocessing] --> B[Model Configuration]
    B --> V[Parameter Recovery Validation]
    V --> C[Model Training]
    C --> D[Posterior Analysis]
    D --> E[Velocity Computation]
    E --> F[Result Visualization]
    F --> M[Model Comparison]
    M --> S[Model Selection]
    S --> G[Integration with Workflows]

    class A,B,V,C,D,E,F,M,S,G scenario;
```

# Usage Model {#sec-usage-model}

## Actors {#sec-actors}

| Actor ID | Actor Name | Description | Related Stakeholder | Primary Goals |
|----------|------------|-------------|---------------------|---------------|
| A-01 | Computational Biologist | Researcher with programming experience who analyzes single-cell data | Computational Biologists | Analyze RNA velocity, infer cell trajectories, quantify uncertainty |
| A-02 | Bioinformatician | Technical specialist who develops and runs analysis pipelines | Bioinformaticians | Integrate RNA velocity into workflows, extend functionality |
| A-03 | Molecular Biologist | Wet-lab researcher who needs to interpret results | Molecular Biologists | Interpret velocity results, design follow-up experiments |
| A-04 | Data Scientist | Researcher focused on method development | Research Community | Develop new models, benchmark against existing methods |
| A-05 | AnnData | Data structure for annotated data matrices | External System | Store and provide access to single-cell data |
| A-06 | Scanpy/scVelo | Single-cell analysis toolkits | External System | Preprocess data, visualize results |

## Usage Scenarios {#sec-usage-scenarios}

| Scenario ID | Scenario Name | Primary Actor | Description | Pre-conditions | Post-conditions |
|-------------|---------------|---------------|-------------|----------------|-----------------|
| US-01 | Data Preprocessing | A-01, A-02 | Prepare single-cell data for velocity analysis | Raw count data available | AnnData object with spliced/unspliced counts |
| US-02 | Model Configuration | A-01, A-04 | Configure model components and parameters | Preprocessed data available | Configured model ready for training |
| US-03 | Parameter Recovery Validation | A-01, A-04 | Validate model's ability to recover known parameters from synthetic data | Model specification and prior distributions available | Validation metrics and visualizations demonstrating parameter recovery performance |
| US-04 | Model Training | A-01, A-02 | Train the model using variational inference or MCMC | Configured model and data available | Trained model with optimized parameters |
| US-05 | Posterior Analysis | A-01, A-04 | Analyze posterior distributions of parameters | Trained model available | Parameter distributions and diagnostics |
| US-06 | Velocity Computation | A-01, A-02 | Compute RNA velocity with uncertainty | Posterior samples available | Velocity estimates with uncertainty |
| US-07 | Result Visualization | A-01, A-03 | Visualize velocity, parameters, and associated uncertainty | Velocity estimates and parameter distributions available | Visualizations for interpretation |
| US-08 | Model Comparison | A-01, A-04 | Systematically compare different model architectures, implementations, and inference algorithms | Multiple trained models available | Comparison metrics and visualizations |
| US-09 | Model Selection | A-01, A-04 | Select the most appropriate model based on validation metrics | Model comparison results available | Selected model for downstream analysis |
| US-10 | Integration with Workflows | A-02 | Integrate velocity analysis and model validation into larger pipelines | Trained model, selected model, and analysis code available | Integrated workflow |

## Functional Scenarios {#sec-functional-scenarios}

**Scenario ID:** FS-01

**Usage Scenario:** US-03 (Parameter Recovery Validation)

**Description:** Validating a PyroVelocity model's ability to recover known parameters from synthetic data to ensure empirically identifiable models

**Triggering Event:** User initiates parameter recovery validation

**Steps:**

1. User specifies model configuration, prior distributions, and inference algorithm (SVI or MCMC)
2. System generates known parameter sets from prior distributions
3. System creates synthetic data using these parameters and the model's generative process
4. System performs inference on synthetic data to estimate parameters
5. System compares estimated parameters to original known values
6. System analyzes recovery accuracy, precision, and coverage
7. System calculates credible intervals and coverage metrics
8. System generates validation metrics and visualizations
9. User evaluates parameter recovery performance
10. System stores validation results for model comparison

**Alternative Paths:**

* At step 1, if user wants to test specific parameter values, they provide custom parameter sets
* At step 1, if user wants to compare inference algorithms, they specify multiple algorithms
* At step 3, if user wants to test robustness, they specify noise levels to add to synthetic data
* At step 4, if user wants to test multiple sample sizes, they specify a range of sample sizes

**Exception Paths:**

* At step 4, if inference fails to converge, system provides diagnostic information
* At step 5, if parameters are not recoverable, system identifies problematic parameters
* At step 7, if credible intervals have poor coverage, system suggests model refinement

**Scenario ID:** FS-02

**Usage Scenario:** US-04 (Model Training)

**Description:** Training a PyroVelocity model on pancreas development data

**Triggering Event:** User initiates model training

**Steps:**

1. User loads preprocessed AnnData object with spliced/unspliced counts
2. User creates a PyroVelocity model with standard components
3. User selects inference algorithm (SVI or MCMC)
4. System initializes model parameters and inference components
5. User calls the train method with the AnnData object
6. If using SVI:
   a. System performs stochastic variational inference
   b. System monitors ELBO convergence and reports progress
7. If using MCMC:
   a. System performs Markov Chain Monte Carlo sampling
   b. System monitors chain diagnostics and reports progress
8. System returns the trained model with posterior samples
9. User saves the trained model for later use

**Alternative Paths:**

* At step 2, if user wants custom components, they configure specific components
* At step 3, if user wants to compare inference algorithms, they specify multiple algorithms
* At step 5, if user wants custom training parameters, they provide additional arguments

**Exception Paths:**

* At step 1, if data is not properly formatted, system provides error message
* At step 6a, if SVI fails to converge, system provides diagnostic information
* At step 7a, if MCMC chains fail to mix, system suggests parameter adjustments

## Common Research Workflows {#sec-common-research-workflows}

**Workflow ID:** WF-01

**Workflow Name:** RNA Velocity Analysis Workflow with Model Validation and Selection

**Description:** Complete workflow for RNA velocity analysis from raw data to interpretation, including parameter recovery validation, model comparison, and model selection

**Participants:** A-01, A-02, A-03, A-04

**Workflow Steps:**

1. Data Preprocessing: US-01
2. Model Configuration: US-02
3. Parameter Recovery Validation: US-03
4. Model Training: US-04
5. Posterior Analysis: US-05
6. Velocity Computation: US-06
7. Result Visualization: US-07
8. Model Comparison: US-08
9. Model Selection: US-09
10. Integration with Workflows: US-10 (optional)

**Variations:**

* For large datasets, users may subsample genes or cells before model training
* For complex biological systems, users may use nonlinear dynamics models with numerical integration
* For integration with other analyses, users may export velocity to other tools
* For comprehensive validation, users may test multiple parameter sets, sample sizes, and noise levels
* For model comparison, users may compare different model architectures, implementations, and inference algorithms
* For inference algorithm comparison, users may compare SVI and MCMC approaches

**Workflow ID:** WF-02

**Workflow Name:** Parameter Recovery Validation and Model Comparison Workflow

**Description:** Focused workflow for validating model's ability to recover known parameters from synthetic data and comparing different model architectures and inference algorithms

**Participants:** A-01, A-04

**Workflow Steps:**

1. Model Configuration: US-02
2. Parameter Recovery Validation: US-03
3. Result Visualization: US-07
4. Model Comparison: US-08
5. Model Selection: US-09

**Variations:**

* For comprehensive validation, users may test multiple parameter sets, sample sizes, and noise levels
* For robustness testing, users may vary sample sizes, noise levels, and data generation processes
* For model development, users may iterate on model design based on validation results
* For inference algorithm comparison, users may compare SVI and MCMC approaches
* For empirical identifiability assessment, users may analyze parameter recovery across different model architectures

# Computational Model {#sec-computational-model}

## Algorithm Specifications {#sec-algorithm-specifications}

**Algorithm ID:** ALG-01

**Algorithm Name:** Standard RNA Velocity Dynamics

**Description:** Standard model of RNA velocity based on first-order kinetics

**Mathematical Formulation:**

```
du/dt = α - βu
ds/dt = βu - γs
```

Where:

- u is the unspliced mRNA abundance
- s is the spliced mRNA abundance
- α is the transcription rate
- β is the splicing rate
- γ is the degradation rate

**Inputs:**

- u0: Initial unspliced RNA counts, tensor[num_cells, num_genes]
- s0: Initial spliced RNA counts, tensor[num_cells, num_genes]
- α: Transcription rates, tensor[num_genes]
- β: Splicing rates, tensor[num_genes]
- γ: Degradation rates, tensor[num_genes]
- τ: Cell-specific time points, tensor[num_cells]

**Outputs:**

- ut: Unspliced RNA counts at time τ, tensor[num_cells, num_genes]
- st: Spliced RNA counts at time τ, tensor[num_cells, num_genes]
- velocity: RNA velocity vectors, tensor[num_cells, num_genes]

**Computational Properties:**

- Time Complexity: O(num_cells * num_genes)
- Space Complexity: O(num_cells * num_genes)
- Numerical Stability: Stable for β ≠ γ, special case handling for β = γ
- Convergence: Converges to steady state (u_ss = α/β, s_ss = α/γ)

**References:**

- La Manno et al. (2018)
- Bergen et al. (2020)

**Algorithm ID:** ALG-02

**Algorithm Name:** Parameter Recovery Validation

**Description:** Validation of model's ability to recover known parameters from synthetic data

**Mathematical Formulation:**

```
Recovery Error = ||θ_true - θ_estimated||
Coverage = P(θ_true ∈ CI(θ_estimated))
```

Where:
- θ_true are the true parameter values
- θ_estimated are the estimated parameter values
- CI(θ_estimated) is the credible interval for the estimated parameters
- ||·|| is a suitable distance metric (e.g., L2 norm)

**Inputs:**

- Model: Probabilistic model specification
- Prior: Prior distributions for parameters
- Num_parameter_sets: Number of parameter sets to test
- Num_samples: Number of synthetic datasets per parameter set
- Sample_sizes: List of sample sizes to test
- Noise_levels: List of noise levels to test

**Outputs:**

- Recovery accuracy metrics (MSE, correlation, etc.)
- Recovery precision metrics (width of credible intervals)
- Coverage metrics (percentage of true parameters within credible intervals)
- Bias metrics (systematic deviation from true values)
- Visualizations of parameter recovery performance

**Computational Properties:**

- Time Complexity: O(num_parameter_sets * num_samples * max_sample_size * num_genes)
- Space Complexity: O(num_parameter_sets * num_samples * max_sample_size * num_genes)
- Parallelization: Embarrassingly parallel across parameter sets and samples

**References:**

- Parameter recovery validation specification
- Principled Bayesian workflow literature

**Algorithm ID:** ALG-03

**Algorithm Name:** Stochastic Variational Inference

**Description:** Approximate Bayesian inference for RNA velocity parameters

**Mathematical Formulation:**

```
ELBO(q) = E_q[log p(x, z)] - E_q[log q(z)]
```

Where:
- p(x, z) is the joint distribution of observed data x and latent variables z
- q(z) is the variational distribution
- ELBO is the Evidence Lower Bound

**Inputs:**

- Model: Probabilistic model p(x, z)
- Guide: Variational distribution q(z)
- Data: Observed data x
- Optimizer: Optimization algorithm
- Num_epochs: Number of training epochs

**Outputs:**

- Optimized guide parameters
- Training diagnostics
- ELBO values
- Posterior samples

**Computational Properties:**

- Time Complexity: O(num_epochs * num_cells * num_genes)
- Space Complexity: O(num_cells * num_genes)
- Convergence: Depends on model complexity and data characteristics
- Parallelization: Embarrassingly parallel across batches

**References:**

- Pyro documentation [@Bingham2018-id]
- Hoffman et al. (2013)

**Algorithm ID:** ALG-04

**Algorithm Name:** Markov Chain Monte Carlo Inference

**Description:** Sampling-based Bayesian inference for RNA velocity parameters

**Mathematical Formulation:**

```
p(z|x) ∝ p(x|z)p(z)
```

Where:
- p(z|x) is the posterior distribution of latent variables z given observed data x
- p(x|z) is the likelihood of observed data given latent variables
- p(z) is the prior distribution of latent variables

**Inputs:**

- Model: Probabilistic model p(x, z)
- Data: Observed data x
- Num_samples: Number of posterior samples
- Num_chains: Number of parallel chains
- Num_warmup: Number of warmup steps

**Outputs:**

- Posterior samples
- Chain diagnostics (R-hat, effective sample size)
- Posterior predictive checks

**Computational Properties:**

- Time Complexity: O(num_samples * num_chains * num_cells * num_genes)
- Space Complexity: O(num_samples * num_chains * num_parameters)
- Convergence: Assessed via chain diagnostics
- Parallelization: Embarrassingly parallel across chains

**References:**

- NumPyro documentation [@Phan2019-numpyro]
- Hoffman and Gelman (2014) [No U-Turn Sampler]

## Computational Interfaces {#sec-computational-interfaces}

**Interface ID:** CI-01

**Algorithm:** ALG-01

**Description:** Interface for dynamics models

**Input Parameters:**

| Parameter | Type | Description | Constraints |
|-----------|------|-------------|------------|
| context | Dict[str, Any] | Context dictionary containing model state | Must contain u_obs, s_obs, and parameters |

**Output Parameters:**

| Parameter | Type | Description |
|-----------|------|-------------|
| context | Dict[str, Any] | Updated context with computed values |

**Error Conditions:**

| Error Code | Description | Recovery Action |
|------------|-------------|----------------|
| ValueError | Missing required parameters | Provide missing parameters |
| RuntimeError | Numerical instability | Check parameter values |

**Interface ID:** CI-02

**Algorithm:** ALG-02

**Description:** Interface for parameter recovery validation

**Input Parameters:**

| Parameter | Type | Description | Constraints |
|-----------|------|-------------|------------|
| model_config | Dict[str, Any] | Model configuration | Must specify model components |
| prior_config | Dict[str, Any] | Prior distribution configuration | Must specify prior distributions for all parameters |
| inference_algorithm | str | Inference algorithm to use (SVI or MCMC) | "SVI" or "MCMC" |
| num_parameter_sets | int | Number of parameter sets to test | > 0 |
| num_samples | int | Number of synthetic datasets per parameter set | > 0 |
| sample_sizes | List[int] | List of sample sizes to test | All values > 0 |
| noise_levels | List[float] | List of noise levels to test | All values >= 0 |
| use_gpu | bool | Whether to use GPU acceleration | True/False |

**Output Parameters:**

| Parameter | Type | Description |
|-----------|------|-------------|
| validation_results | Dict[str, Any] | Validation metrics and results |
| parameter_recovery_plots | Dict[str, Figure] | Visualizations of parameter recovery |
| recovery_summary | Dict[str, float] | Summary statistics of recovery performance |
| credible_interval_coverage | Dict[str, float] | Coverage statistics for credible intervals |

**Error Conditions:**

| Error Code | Description | Recovery Action |
|------------|-------------|----------------|
| ValueError | Invalid model or prior configuration | Correct configuration |
| RuntimeError | Inference failure | Adjust inference parameters or model complexity |
| MemoryError | Out of memory | Reduce number of parameter sets or sample sizes |
| ConvergenceError | Inference algorithm fails to converge | Adjust inference parameters or try alternative algorithm |

**Interface ID:** CI-03

**Algorithm:** ALG-03, ALG-04

**Description:** Interface for model training

**Input Parameters:**

| Parameter | Type | Description | Constraints |
|-----------|------|-------------|------------|
| adata | AnnData | Annotated data object | Must contain spliced/unspliced layers |
| inference_algorithm | str | Inference algorithm to use (SVI or MCMC) | "SVI" or "MCMC" |
| max_epochs | int | Maximum number of training epochs (SVI) or samples (MCMC) | > 0 |
| batch_size | Optional[int] | Batch size for training (SVI only) | None or > 0 |
| learning_rate | float | Learning rate for optimizer (SVI only) | > 0 |
| num_chains | int | Number of MCMC chains (MCMC only) | > 0 |
| num_warmup | int | Number of warmup steps (MCMC only) | > 0 |
| use_gpu | bool | Whether to use GPU acceleration | True/False |

**Output Parameters:**

| Parameter | Type | Description |
|-----------|------|-------------|
| model | PyroVelocityModel | Trained model |
| loss_history | List[float] | Training loss history (SVI only) |
| posterior_samples | Dict[str, Tensor] | Posterior samples |
| diagnostics | Dict[str, Any] | Training or sampling diagnostics |

**Error Conditions:**

| Error Code | Description | Recovery Action |
|------------|-------------|----------------|
| ValueError | Invalid AnnData format | Preprocess data correctly |
| RuntimeError | Training divergence or sampling failure | Adjust parameters or try alternative algorithm |
| MemoryError | Out of memory | Reduce batch size, number of chains, or use data subsampling |
| ConvergenceError | Inference algorithm fails to converge | Adjust inference parameters or try alternative algorithm |

## Execution Model {#sec-execution-model}

**Execution Model ID:** EM-01

**Description:** Execution model for PyroVelocity

**Execution Environment:** Python environment with PyTorch/Pyro or JAX/NumPyro

**Execution Strategy:**

- Parallelization: Data parallelism across cells and genes
- Distribution: Single-node execution with optional GPU acceleration
- Scheduling: Batch-based processing for large datasets
- Inference: Multiple inference algorithms (SVI and MCMC)

**Resource Requirements:**

- Compute: Multi-core CPU or CUDA-compatible GPU
- Memory: 16-64GB RAM depending on dataset size and inference algorithm
- Storage: 10-100GB for datasets, results, and posterior samples
- Network: Minimal (data loading only)

**Scalability Characteristics:**

- Scales linearly with number of cells and genes
- GPU acceleration provides significant speedup for large datasets
- JAX/NumPyro implementation offers improved performance over PyTorch/Pyro
- MCMC requires more computational resources than SVI but provides more accurate uncertainty quantification
- Parallel chains in MCMC can leverage multi-GPU systems

## Performance Parameters {#sec-performance-parameters}

| Algorithm ID | Parameter | Description | Target Value | Measurement Method |
|--------------|-----------|-------------|--------------|-------------------|
| ALG-01 | Execution Time | Time to compute velocity for 10k cells, 2k genes | < 1 minute on GPU | Wall clock time |
| ALG-02 | Training Time | Time to train model on 10k cells, 2k genes | < 30 minutes on GPU | Wall clock time |
| ALG-02 | Memory Usage | Peak memory usage during training | < 16GB | Process monitoring |
| ALG-03 | Convergence Rate (SVI) | Number of epochs to reach convergence | < 1000 epochs | ELBO monitoring |
| ALG-04 | Sampling Time (MCMC) | Time to generate 1000 posterior samples with 4 chains | < 2 hours on GPU | Wall clock time |
| ALG-04 | Memory Usage (MCMC) | Peak memory usage during MCMC sampling | < 32GB | Process monitoring |
| ALG-04 | Chain Convergence | R-hat statistic for all parameters | < 1.1 | MCMC diagnostics |
| ALG-04 | Effective Sample Size | Minimum effective sample size across parameters | > 100 | MCMC diagnostics |

# Data Model {#sec-data-model}

## Data Objects {#sec-data-objects}

| Data Object ID | Data Object Name | Description | Related Scientific Data Object | Persistence |
|----------------|------------------|-------------|-------------------------------|------------|
| DO-01 | AnnData | Annotated data matrix for single-cell data | AnnData | Yes |
| DO-02 | CountMatrix | Matrix of RNA counts | CountMatrix | Yes |
| DO-03 | ModelParameters | Parameters of the RNA velocity model | ModelParameters | Yes |
| DO-04 | PosteriorSamples | Samples from parameter posterior distributions | PosteriorSamples | Yes |
| DO-05 | VelocityEstimates | RNA velocity vectors for each cell | VelocityEstimates | Yes |
| DO-06 | ModelState | Immutable state container for model components | N/A | Yes |
| DO-07 | ComponentConfig | Configuration for model components | N/A | Yes |
| DO-08 | SyntheticData | Synthetic data generated with known parameters | N/A | Yes |
| DO-09 | ParameterRecoveryResults | Results of parameter recovery validation | N/A | Yes |
| DO-10 | ModelComparisonResults | Results of comparing different model architectures and inference algorithms | N/A | Yes |
| DO-11 | MCMCDiagnostics | Diagnostics for MCMC sampling | N/A | Yes |
| DO-12 | CredibleIntervals | Credible intervals for parameter estimates | N/A | Yes |
| DO-13 | ValidationMetrics | Metrics for validating model performance | N/A | Yes |

## Data Object Attributes {#sec-data-object-attributes}

**Data Object:** DO-01 (AnnData)

| Attribute ID | Attribute Name | Description | Type | Units | Constraints | Default Value |
|--------------|----------------|-------------|------|-------|-------------|---------------|
| ATTR-01 | X | Gene expression matrix | Matrix | Counts | Non-negative | None |
| ATTR-02 | layers["spliced"] | Spliced RNA counts | Matrix | Counts | Non-negative | None |
| ATTR-03 | layers["unspliced"] | Unspliced RNA counts | Matrix | Counts | Non-negative | None |
| ATTR-04 | obs | Cell annotations | DataFrame | Various | None | None |
| ATTR-05 | var | Gene annotations | DataFrame | Various | None | None |
| ATTR-06 | obsm | Cell embeddings | Dict[str, Matrix] | Various | None | None |
| ATTR-07 | uns | Unstructured annotations | Dict | Various | None | None |

**Data Object:** DO-03 (ModelParameters)

| Attribute ID | Attribute Name | Description | Type | Units | Constraints | Default Value |
|--------------|----------------|-------------|------|-------|-------------|---------------|
| ATTR-08 | alpha | Transcription rates | Tensor | Transcripts per unit time | Non-negative | None |
| ATTR-09 | beta | Splicing rates | Tensor | Per unit time | Non-negative | None |
| ATTR-10 | gamma | Degradation rates | Tensor | Per unit time | Non-negative | None |
| ATTR-11 | switching | Switching times | Tensor | Time | Non-negative | None |
| ATTR-12 | t | Cell-specific times | Tensor | Time | Non-negative | None |
| ATTR-13 | u_scale | Scaling factors for unspliced counts | Tensor | Dimensionless | Positive | 1.0 |
| ATTR-14 | s_scale | Scaling factors for spliced counts | Tensor | Dimensionless | Positive | 1.0 |

**Data Object:** DO-08 (SyntheticData)

| Attribute ID | Attribute Name | Description | Type | Units | Constraints | Default Value |
|--------------|----------------|-------------|------|-------|-------------|---------------|
| ATTR-15 | u_obs | Observed unspliced counts | Tensor | Counts | Non-negative | None |
| ATTR-16 | s_obs | Observed spliced counts | Tensor | Counts | Non-negative | None |
| ATTR-17 | true_parameters | True parameter values used to generate data | Dict[str, Tensor] | Various | Non-negative | None |
| ATTR-18 | sample_size | Number of cells in the synthetic dataset | int | Count | > 0 | None |
| ATTR-19 | noise_level | Noise level used in data generation | float | Dimensionless | >= 0 | 0.0 |
| ATTR-20 | seed | Random seed used for reproducibility | int | Dimensionless | Any | None |

**Data Object:** DO-09 (ParameterRecoveryResults)

| Attribute ID | Attribute Name | Description | Type | Units | Constraints | Default Value |
|--------------|----------------|-------------|------|-------|-------------|---------------|
| ATTR-21 | true_parameters | True parameter values | Dict[str, Tensor] | Various | Non-negative | None |
| ATTR-22 | estimated_parameters | Estimated parameter values | Dict[str, Tensor] | Various | Non-negative | None |
| ATTR-23 | credible_intervals | Credible intervals for estimated parameters | Dict[str, Tuple[Tensor, Tensor]] | Various | Lower < Upper | None |
| ATTR-24 | recovery_metrics | Metrics quantifying parameter recovery performance | Dict[str, Dict[str, float]] | Various | None | None |
| ATTR-25 | coverage_metrics | Metrics quantifying credible interval coverage | Dict[str, float] | Percentage | [0, 100] | None |
| ATTR-26 | bias_metrics | Metrics quantifying systematic bias in estimates | Dict[str, float] | Various | None | None |
| ATTR-27 | sample_size | Number of cells used in recovery | int | Count | > 0 | None |
| ATTR-28 | noise_level | Noise level used in data generation | float | Dimensionless | >= 0 | 0.0 |
| ATTR-29 | inference_algorithm | Inference algorithm used | str | N/A | "SVI" or "MCMC" | "SVI" |
| ATTR-30 | model_architecture | Model architecture used | str | N/A | Valid model name | "standard" |
| ATTR-31 | empirical_identifiability | Assessment of parameter identifiability | Dict[str, bool] | N/A | True/False | None |

**Data Object:** DO-10 (ModelComparisonResults)

| Attribute ID | Attribute Name | Description | Type | Units | Constraints | Default Value |
|--------------|----------------|-------------|------|-------|-------------|---------------|
| ATTR-32 | model_configs | Configurations of compared models | List[Dict[str, Any]] | N/A | Non-empty | None |
| ATTR-33 | inference_algorithms | Inference algorithms used | List[str] | N/A | "SVI" or "MCMC" | None |
| ATTR-34 | performance_metrics | Performance metrics for each model | Dict[str, Dict[str, float]] | Various | None | None |
| ATTR-35 | parameter_recovery_metrics | Parameter recovery metrics for each model | Dict[str, Dict[str, float]] | Various | None | None |
| ATTR-36 | uncertainty_calibration | Uncertainty calibration metrics for each model | Dict[str, Dict[str, float]] | Various | None | None |
| ATTR-37 | computational_metrics | Computational performance metrics | Dict[str, Dict[str, float]] | Various | None | None |
| ATTR-38 | selected_model | Selected model based on comparison | str | N/A | Valid model name | None |
| ATTR-39 | selection_criteria | Criteria used for model selection | Dict[str, float] | Various | None | None |

**Data Object:** DO-11 (MCMCDiagnostics)

| Attribute ID | Attribute Name | Description | Type | Units | Constraints | Default Value |
|--------------|----------------|-------------|------|-------|-------------|---------------|
| ATTR-40 | r_hat | R-hat convergence statistic | Dict[str, float] | Dimensionless | > 0 | None |
| ATTR-41 | effective_sample_size | Effective sample size | Dict[str, float] | Count | > 0 | None |
| ATTR-42 | chain_statistics | Statistics for each chain | Dict[str, Dict[str, Any]] | Various | None | None |
| ATTR-43 | acceptance_rate | MCMC acceptance rate | float | Percentage | [0, 100] | None |
| ATTR-44 | divergences | Number of divergent transitions | int | Count | >= 0 | None |
| ATTR-45 | energy_statistics | Energy statistics for HMC | Dict[str, float] | Various | None | None |

## Data Object Relationships {#sec-data-object-relationships}

| Relationship ID | From Object | To Object | Type | Cardinality | Description |
|-----------------|-------------|-----------|------|-------------|-------------|
| REL-01 | AnnData | CountMatrix | Composition | 1:N | AnnData contains multiple count matrices (X, layers) |
| REL-02 | PyroVelocityModel | ModelParameters | Association | 1:1 | Model uses parameters for computation |
| REL-03 | PyroVelocityModel | PosteriorSamples | Association | 1:1 | Model generates posterior samples |
| REL-04 | PosteriorSamples | VelocityEstimates | Derivation | 1:1 | Velocity is computed from posterior samples |
| REL-05 | AnnData | VelocityEstimates | Association | 1:1 | Velocity estimates are stored in AnnData |
| REL-06 | PyroVelocityModel | ModelState | Composition | 1:1 | Model contains state |
| REL-07 | ModelState | ComponentConfig | Composition | 1:N | State contains component configurations |
| REL-08 | ModelParameters | SyntheticData | Generation | 1:N | Parameters are used to generate synthetic data |
| REL-09 | SyntheticData | ParameterRecoveryResults | Association | 1:1 | Synthetic data is used for parameter recovery validation |
| REL-10 | ParameterRecoveryResults | ModelParameters | Validation | N:1 | Recovery results validate parameter inference |
| REL-11 | PosteriorSamples | MCMCDiagnostics | Association | 1:1 | MCMC diagnostics are generated from posterior samples |
| REL-12 | PosteriorSamples | CredibleIntervals | Derivation | 1:1 | Credible intervals are computed from posterior samples |
| REL-13 | ParameterRecoveryResults | ModelComparisonResults | Association | N:1 | Parameter recovery results are used for model comparison |
| REL-14 | ModelComparisonResults | ValidationMetrics | Composition | 1:1 | Model comparison results contain validation metrics |
| REL-15 | PyroVelocityModel | ModelComparisonResults | Association | N:1 | Multiple models are compared in model comparison |

```{mermaid}
classDiagram
    AnnData *-- CountMatrix : contains
    PyroVelocityModel o-- ModelParameters : uses
    PyroVelocityModel --> PosteriorSamples : generates
    PosteriorSamples --> VelocityEstimates : computes
    AnnData o-- VelocityEstimates : stores
    PyroVelocityModel *-- ModelState : contains
    ModelState *-- ComponentConfig : contains
    ModelParameters --> SyntheticData : generates
    SyntheticData --> ParameterRecoveryResults : validates
    ParameterRecoveryResults --> ModelParameters : validates
    PosteriorSamples --> MCMCDiagnostics : generates
    PosteriorSamples --> CredibleIntervals : computes
    ParameterRecoveryResults --> ModelComparisonResults : contributes to
    ModelComparisonResults *-- ValidationMetrics : contains
    PyroVelocityModel --> ModelComparisonResults : compared in

    class AnnData {
        +Matrix X
        +Dict layers
        +DataFrame obs
        +DataFrame var
    }

    class PyroVelocityModel {
        +DynamicsModel dynamics_model
        +PriorModel prior_model
        +LikelihoodModel likelihood_model
        +ObservationModel observation_model
        +InferenceGuide guide_model
        +ModelState state
        +str inference_algorithm
    }

    class ModelParameters {
        +Tensor alpha
        +Tensor beta
        +Tensor gamma
        +Tensor switching
        +Tensor t
    }

    class PosteriorSamples {
        +Dict parameter_samples
        +int num_samples
        +int num_chains
        +str inference_algorithm
    }

    class VelocityEstimates {
        +Matrix velocity
        +Matrix uncertainty
    }

    class ModelState {
        +Dict dynamics_state
        +Dict prior_state
        +Dict likelihood_state
        +Dict observation_state
        +Dict guide_state
    }

    class ComponentConfig {
        +str name
        +Dict params
    }

    class SyntheticData {
        +Tensor u_obs
        +Tensor s_obs
        +Dict true_parameters
        +int sample_size
        +float noise_level
    }

    class ParameterRecoveryResults {
        +Dict true_parameters
        +Dict estimated_parameters
        +Dict credible_intervals
        +Dict recovery_metrics
        +Dict coverage_metrics
        +str inference_algorithm
        +str model_architecture
        +Dict empirical_identifiability
    }

    class ModelComparisonResults {
        +List model_configs
        +List inference_algorithms
        +Dict performance_metrics
        +Dict parameter_recovery_metrics
        +str selected_model
        +Dict selection_criteria
    }

    class MCMCDiagnostics {
        +Dict r_hat
        +Dict effective_sample_size
        +float acceptance_rate
        +int divergences
    }

    class CredibleIntervals {
        +Dict intervals
        +Dict coverage
    }

    class ValidationMetrics {
        +Dict metrics
        +Dict criteria
    }
```

## Data Management Requirements {#sec-data-management-requirements}

| Requirement ID | Requirement Name | Description | Priority |
|----------------|------------------|-------------|----------|
| DM-01 | AnnData Compatibility | System must maintain compatibility with AnnData format | High |
| DM-02 | Serialization | System must support serialization of models and results | High |
| DM-03 | Versioning | System must track model and data versions | Medium |
| DM-04 | Data Provenance | System must track data processing steps | Medium |
| DM-05 | Large Dataset Support | System must support large datasets through chunking or streaming | Medium |

**Data Volume Considerations:**

- Expected data volume: 1-100GB per dataset
- Growth rate: Linear with number of cells and genes
- Retention period: Indefinite for published results

**Data Access Patterns:**

- Random access to cells and genes during training
- Sequential access to posterior samples during analysis
- Batch access to cells during visualization

**Data Quality Requirements:**

- Validation of input data format and content
- Handling of missing values and zeros
- Detection and reporting of data quality issues

# Functional Requirements {#sec-functional-requirements}

## Core Functions {#sec-core-functions}

| Function ID | Function Name | Description | Related Usage Scenarios | Priority |
|-------------|---------------|-------------|-------------------------|----------|
| FN-01 | Data Preprocessing | Prepare single-cell data for velocity analysis | US-01 | High |
| FN-02 | Model Configuration | Configure model components and parameters | US-02 | High |
| FN-03 | Parameter Recovery Validation | Validate model's ability to recover known parameters from synthetic data | US-03 | High |
| FN-04 | Model Training | Train the model using variational inference or MCMC | US-04 | High |
| FN-05 | Posterior Sampling | Generate samples from posterior distributions | US-05 | High |
| FN-06 | Velocity Computation | Compute RNA velocity with uncertainty | US-06 | High |
| FN-07 | Result Visualization | Visualize velocity, parameters, and associated uncertainty | US-07 | High |
| FN-08 | Model Comparison | Systematically compare different model architectures, implementations, and inference algorithms | US-08 | High |
| FN-09 | Model Selection | Select the most appropriate model based on validation metrics | US-09 | Medium |
| FN-10 | Model Serialization | Save and load trained models | US-04, US-05 | Medium |
| FN-11 | AnnData Integration | Store results in AnnData objects | US-01, US-06, US-07 | High |
| FN-12 | Parameter Analysis | Analyze posterior distributions of parameters | US-05 | Medium |
| FN-13 | Trajectory Inference | Infer cell trajectories from velocity | US-06, US-07 | Medium |

## Function Hierarchy {#sec-function-hierarchy}

```{mermaid}
graph TD
    FN01[FN-01: Data Preprocessing] --> FN02[FN-02: Model Configuration]
    FN02 --> FN03[FN-03: Parameter Recovery Validation]
    FN03 --> FN04[FN-04: Model Training]
    FN04 --> FN05[FN-05: Posterior Sampling]
    FN05 --> FN06[FN-06: Velocity Computation]
    FN06 --> FN07[FN-07: Result Visualization]
    FN03 --> FN08[FN-08: Model Comparison]
    FN04 --> FN08
    FN08 --> FN09[FN-09: Model Selection]
    FN04 --> FN10[FN-10: Model Serialization]
    FN01 --> FN11[FN-11: AnnData Integration]
    FN06 --> FN11
    FN05 --> FN12[FN-12: Parameter Analysis]
    FN06 --> FN13[FN-13: Trajectory Inference]
    FN13 --> FN07
    FN12 --> FN07
    FN12 --> FN08

    class FN01,FN02,FN03,FN04,FN05,FN06,FN07,FN08,FN09,FN10,FN11,FN12,FN13 function;
```

## Function Input/Output Specifications {#sec-function-inputoutput-specifications}

**Function:** FN-03 (Parameter Recovery Validation)

**Inputs:**

| Input Name | Type | Description | Source | Constraints |
|------------|------|-------------|--------|------------|
| model_config | ModelConfig | Model configuration | FN-02 | Valid component configurations |
| prior_config | Dict | Prior distribution configuration | User | Valid prior distributions |
| inference_algorithm | str | Inference algorithm to use | User | "SVI" or "MCMC" |
| num_parameter_sets | int | Number of parameter sets to test | User | > 0 |
| sample_sizes | List[int] | List of sample sizes to test | User | All values > 0 |
| noise_levels | List[float] | List of noise levels to test | User | All values >= 0 |

**Outputs:**

| Output Name | Type | Description | Destination | Constraints |
|-------------|------|-------------|------------|------------|
| validation_results | Dict | Validation metrics and results | FN-08, FN-09 | None |
| parameter_recovery_plots | Dict | Visualizations of parameter recovery | FN-07 | None |
| recovery_summary | Dict | Summary statistics of recovery performance | FN-08, FN-09 | None |
| credible_interval_coverage | Dict | Coverage statistics for credible intervals | FN-08, FN-09 | None |

**Function:** FN-04 (Model Training)

**Inputs:**

| Input Name | Type | Description | Source | Constraints |
|------------|------|-------------|--------|------------|
| adata | AnnData | Annotated data object | FN-01 | Must contain spliced/unspliced layers |
| model_config | ModelConfig | Model configuration | FN-02 | Valid component configurations |
| inference_algorithm | str | Inference algorithm to use | User | "SVI" or "MCMC" |
| training_params | Dict | Training parameters | User | Valid parameter values |

**Outputs:**

| Output Name | Type | Description | Destination | Constraints |
|-------------|------|-------------|------------|------------|
| trained_model | PyroVelocityModel | Trained model | FN-05, FN-06, FN-10 | None |
| loss_history | List[float] | Training loss history (SVI only) | FN-07 | None |
| diagnostics | Dict | Training or sampling diagnostics | FN-07, FN-08 | None |
| posterior_samples | Dict | Posterior samples | FN-05, FN-12 | None |

**Function:** FN-06 (Velocity Computation)

**Inputs:**

| Input Name | Type | Description | Source | Constraints |
|------------|------|-------------|--------|------------|
| trained_model | PyroVelocityModel | Trained model | FN-04, FN-10 | None |
| posterior_samples | Dict | Posterior samples | FN-05 | None |
| adata | AnnData | Annotated data object | FN-01 | Must contain spliced/unspliced layers |

**Outputs:**

| Output Name | Type | Description | Destination | Constraints |
|-------------|------|-------------|------------|------------|
| velocity | Matrix | Velocity vectors | FN-07, FN-11, FN-13 | None |
| velocity_uncertainty | Matrix | Uncertainty estimates | FN-07, FN-11 | None |
| velocity_metrics | Dict | Velocity quality metrics | FN-07, FN-11 | None |

**Function:** FN-08 (Model Comparison)

**Inputs:**

| Input Name | Type | Description | Source | Constraints |
|------------|------|-------------|--------|------------|
| model_configs | List[ModelConfig] | Configurations of compared models | FN-02 | Non-empty list |
| validation_results | List[Dict] | Validation results for each model | FN-03 | None |
| trained_models | List[PyroVelocityModel] | Trained models to compare | FN-04 | None |
| parameter_analysis | List[Dict] | Parameter analysis results | FN-12 | None |
| comparison_metrics | Dict | Metrics to use for comparison | User | Valid metric specifications |

**Outputs:**

| Output Name | Type | Description | Destination | Constraints |
|-------------|------|-------------|------------|------------|
| comparison_results | Dict | Comparison metrics and results | FN-09, FN-07 | None |
| comparison_plots | Dict | Visualizations of model comparison | FN-07 | None |
| model_rankings | Dict | Rankings of models by different criteria | FN-09 | None |

## Function Relationships {#sec-function-relationships}

| From Function | To Function | Relationship Type | Description |
|---------------|-------------|-------------------|-------------|
| FN-01 | FN-02 | PROVIDES_DATA_TO | Preprocessed data is used for model configuration |
| FN-02 | FN-03 | PROVIDES_CONFIG_TO | Model configuration is used for parameter recovery validation |
| FN-02 | FN-04 | PROVIDES_CONFIG_TO | Model configuration is used for model training |
| FN-03 | FN-08 | PROVIDES_DATA_TO | Validation results are used for model comparison |
| FN-03 | FN-09 | PROVIDES_DATA_TO | Validation results are used for model selection |
| FN-04 | FN-05 | ENABLES | Trained model is used for posterior sampling |
| FN-04 | FN-08 | PROVIDES_DATA_TO | Trained models are used for model comparison |
| FN-04 | FN-10 | PROVIDES_DATA_TO | Trained model is serialized |
| FN-05 | FN-06 | PROVIDES_DATA_TO | Posterior samples are used for velocity computation |
| FN-05 | FN-12 | PROVIDES_DATA_TO | Posterior samples are used for parameter analysis |
| FN-06 | FN-07 | PROVIDES_DATA_TO | Velocity estimates are used for visualization |
| FN-06 | FN-11 | PROVIDES_DATA_TO | Velocity estimates are stored in AnnData |
| FN-06 | FN-13 | PROVIDES_DATA_TO | Velocity estimates are used for trajectory inference |
| FN-08 | FN-09 | PROVIDES_DATA_TO | Model comparison results are used for model selection |
| FN-10 | FN-04 | PROVIDES_DATA_TO | Serialized model is loaded for continued training |
| FN-12 | FN-07 | PROVIDES_DATA_TO | Parameter analysis results are used for visualization |
| FN-12 | FN-08 | PROVIDES_DATA_TO | Parameter analysis results are used for model comparison |
| FN-13 | FN-07 | PROVIDES_DATA_TO | Trajectory inference results are used for visualization |

# Quality Requirements {#sec-quality-requirements}

## Accuracy and Precision {#sec-accuracy-and-precision}

| Requirement ID | Requirement Name | Description | Verification Method | Target Value |
|----------------|------------------|-------------|---------------------|--------------|
| ACC-01 | Parameter Estimation Accuracy | Accuracy of inferred parameters on synthetic data | Comparison with ground truth | Within 10% of true values |
| ACC-02 | Velocity Direction Accuracy | Accuracy of velocity direction on benchmark datasets | Comparison with known trajectories | > 80% agreement |
| ACC-03 | Uncertainty Calibration | Calibration of uncertainty estimates | Coverage analysis | 95% credible intervals contain true values in > 90% of cases |
| ACC-04 | Numerical Stability | Stability of numerical computations | Stress testing with extreme values | No NaN or Inf values in results |
| ACC-05 | Model Comparison Accuracy | Accuracy of model comparison metrics | Simulation studies | Correct model selection in > 90% of cases |
| ACC-06 | Empirical Identifiability | Identifiability of model parameters | Parameter recovery validation | > 90% of parameters recoverable within specified bounds |
| ACC-07 | Inference Algorithm Accuracy | Accuracy of different inference algorithms | Comparison with analytical solutions | MCMC more accurate than SVI for complex posteriors |

**Validation Approach:**

- Synthetic data with known ground truth
- Benchmark datasets with established trajectories
- Cross-validation with held-out data
- Comparison with deterministic methods
- Simulation studies with known model differences
- Parameter recovery validation across multiple sample sizes
- Comparison of SVI and MCMC inference algorithms

**Error Bounds:**

- Parameter estimation: 95% credible intervals
- Velocity direction: Angular error < 30 degrees
- Trajectory inference: Pseudotime correlation > 0.8
- Model selection: Bayes factor > 3 for significant difference
- Parameter recovery: Within 2 standard deviations of true value

## Performance Requirements {#sec-performance-requirements}

| Requirement ID | Requirement Name | Description | Measurement Method | Target Value |
|----------------|------------------|-------------|-------------------|-------------|
| PERF-01 | SVI Training Time | Time to train model using SVI on standard dataset | Wall clock time | < 30 minutes on GPU |
| PERF-02 | MCMC Sampling Time | Time to generate MCMC samples on standard dataset | Wall clock time | < 2 hours on GPU |
| PERF-03 | Memory Usage (SVI) | Peak memory usage during SVI training | Process monitoring | < 16GB for 10k cells |
| PERF-04 | Memory Usage (MCMC) | Peak memory usage during MCMC sampling | Process monitoring | < 32GB for 10k cells |
| PERF-05 | Posterior Sampling Speed | Time to generate posterior samples from trained model | Wall clock time | < 5 minutes for 1000 samples |
| PERF-06 | Velocity Computation Speed | Time to compute velocity from samples | Wall clock time | < 1 minute for 10k cells |
| PERF-07 | Visualization Speed | Time to generate standard visualizations | Wall clock time | < 30 seconds per plot |
| PERF-08 | Parameter Recovery Validation Speed | Time to complete parameter recovery validation | Wall clock time | < 1 hour per model configuration |
| PERF-09 | Model Comparison Speed | Time to compare multiple model configurations | Wall clock time | < 10 minutes per comparison |

**Scalability Requirements:**

- Linear scaling with number of cells up to 100k cells
- Linear scaling with number of genes up to 5k genes
- Sublinear scaling with number of posterior samples
- Linear scaling with number of MCMC chains
- Linear scaling with number of model configurations in comparison

**Resource Utilization Limits:**

- CPU: < 16 cores at 100% utilization
- GPU: < 32GB VRAM
- Disk: < 100GB for intermediate results
- Memory: < 64GB RAM for MCMC with multiple chains

## Reliability Requirements {#sec-reliability-requirements}

| Requirement ID | Requirement Name | Description | Verification Method | Target Value |
|----------------|------------------|-------------|---------------------|-------------|
| REL-01 | SVI Convergence | Reliable convergence of SVI training process | Success rate on benchmark datasets | > 95% success rate |
| REL-02 | MCMC Convergence | Reliable convergence of MCMC chains | R-hat statistics | R-hat < 1.1 for all parameters |
| REL-03 | Numerical Robustness | Robustness to numerical issues | Testing with varied datasets | No failures due to numerical issues |
| REL-04 | Error Handling | Proper handling of errors and edge cases | Exception testing | All errors properly caught and reported |
| REL-05 | State Persistence | Reliable saving and loading of model state | Recovery testing | 100% successful recovery |
| REL-06 | Parameter Recovery Reliability | Reliable parameter recovery across different models | Success rate on synthetic data | > 90% success rate |
| REL-07 | Model Comparison Reliability | Reliable model comparison results | Consistency testing | Consistent rankings across multiple runs |

**Fault Tolerance Approach:**

- Automatic parameter initialization retry on failure
- Graceful degradation with problematic genes
- Checkpointing during long-running operations
- Validation of inputs before computation
- Automatic adaptation of MCMC parameters
- Fallback to alternative inference algorithms
- Robust model comparison metrics

**Data Integrity Requirements:**

- Validation of AnnData objects before processing
- Checksums for serialized models
- Version tracking for models and data
- Validation of MCMC diagnostics
- Validation of parameter recovery results
- Validation of model comparison metrics

## Usability Requirements {#sec-usability-requirements}

| Requirement ID | Requirement Name | Description | Verification Method | Target Value |
|----------------|------------------|-------------|---------------------|-------------|
| USE-01 | API Simplicity | Simplicity of core API for common tasks | User testing | < 5 lines of code for standard workflow |
| USE-02 | Documentation Quality | Comprehensive and clear documentation | Documentation coverage | 100% API coverage with examples |
| USE-03 | Error Messages | Clear and actionable error messages | User testing | > 90% of errors resolvable from message |
| USE-04 | Visualization Quality | Quality and interpretability of visualizations | User feedback | > 80% positive feedback |
| USE-05 | Integration Ease | Ease of integration with existing workflows | Integration testing | Compatible with scanpy/scVelo workflows |
| USE-06 | Parameter Recovery API | Simplicity of parameter recovery validation API | User testing | < 10 lines of code for standard validation |
| USE-07 | Model Comparison API | Simplicity of model comparison API | User testing | < 10 lines of code for standard comparison |
| USE-08 | Inference Algorithm Selection | Ease of selecting and configuring inference algorithms | User testing | Single parameter to switch between SVI and MCMC |
| USE-09 | Diagnostic Visualization | Quality of diagnostic visualizations for model validation | User feedback | > 80% positive feedback |
| USE-10 | Model Selection Guidance | Clear guidance for model selection based on validation metrics | User feedback | > 80% of users can select appropriate model |

## Reproducibility Requirements {#sec-reproducibility-requirements}

| Requirement ID | Requirement Name | Description | Verification Method | Target Value |
|----------------|------------------|-------------|---------------------|-------------|
| REP-01 | Seed Control | Control over random seeds for reproducibility | Reproducibility testing | Identical results with same seed |
| REP-02 | Version Tracking | Tracking of software and data versions | Version inspection | All components versioned |
| REP-03 | Parameter Logging | Logging of all parameters and configurations | Log inspection | All parameters recorded |
| REP-04 | Environment Specification | Specification of computational environment | Environment testing | Reproducible environment |
| REP-05 | Workflow Integration | Integration with workflow management systems | Workflow testing | Compatible with standard workflow systems |
| REP-06 | MCMC Diagnostics | Recording of MCMC diagnostics for reproducibility | Diagnostic inspection | Complete diagnostics recorded |
| REP-07 | Parameter Recovery Tracking | Tracking of parameter recovery validation results | Result inspection | Complete validation results recorded |
| REP-08 | Model Comparison Tracking | Tracking of model comparison results | Result inspection | Complete comparison results recorded |
| REP-09 | Inference Algorithm Tracking | Tracking of inference algorithm configurations | Configuration inspection | Complete algorithm configurations recorded |
| REP-10 | Validation Dataset Versioning | Versioning of synthetic datasets used for validation | Dataset inspection | All validation datasets versioned |

**Reproducibility Mechanisms:**

- Explicit random seed control
- Deterministic computation modes
- Configuration serialization
- Comprehensive logging
- MCMC chain initialization tracking
- Parameter recovery validation result serialization
- Model comparison result serialization
- Inference algorithm configuration serialization
- Synthetic dataset serialization

**Provenance Requirements:**

- Recording of data preprocessing steps
- Tracking of model configurations
- Logging of training parameters
- Documentation of analysis decisions
- Recording of parameter recovery validation configurations
- Recording of model comparison configurations
- Recording of inference algorithm configurations
- Recording of synthetic dataset generation parameters

# System Constraints {#sec-system-constraints}

## Scientific Constraints {#sec-scientific-constraints}

| Constraint ID | Constraint Name | Description | Source | Impact |
|---------------|-----------------|-------------|--------|--------|
| SC-01 | RNA Velocity Model Assumptions | The model assumes first-order kinetics for transcription, splicing, and degradation | RNA velocity theory | Limits ability to model complex regulatory dynamics |
| SC-02 | Steady State Assumption | The model assumes cells can reach steady state | RNA velocity theory | May not accurately model highly dynamic processes |
| SC-03 | Parameter Identifiability | Not all parameters may be uniquely identifiable from the data | Statistical theory | Requires careful prior specification and validation |
| SC-04 | Data Quality Requirements | Requires high-quality spliced and unspliced count data | Single-cell sequencing | Limits applicability to certain datasets |
| SC-05 | Biological Variability | Biological systems exhibit inherent variability | Biology | Requires probabilistic modeling approach |
| SC-06 | Model Comparison Limitations | Model comparison metrics may not always select the biologically most relevant model | Statistical theory | Requires domain knowledge for final model selection |
| SC-07 | Inference Algorithm Tradeoffs | Different inference algorithms have different strengths and weaknesses | Statistical theory | Requires careful selection based on model complexity |
| SC-08 | Synthetic Data Limitations | Synthetic data may not capture all aspects of real biological data | Simulation theory | Validation results may not fully translate to real data |
| SC-09 | Empirical Identifiability | Empirical identifiability depends on sample size and noise level | Statistical theory | Requires validation across multiple conditions |
| SC-10 | Model Misspecification | All models are approximations of the true biological process | Modeling theory | Multiple models may be needed for robust inference |

## Technical Constraints {#sec-technical-constraints}

| Constraint ID | Constraint Name | Description | Source | Impact |
|---------------|-----------------|-------------|--------|--------|
| TC-01 | AnnData Compatibility | Must maintain compatibility with AnnData format | Ecosystem integration | Influences data structure design |
| TC-02 | Pyro/PyTorch Dependency | Relies on Pyro and PyTorch for probabilistic programming | Implementation choice | Affects performance and deployment options |
| TC-03 | JAX/NumPyro Dependency | Relies on JAX and NumPyro for alternative implementation | Implementation choice | Affects performance and deployment options |
| TC-04 | Python Ecosystem | Must integrate with Python-based single-cell analysis tools | Ecosystem integration | Limits language and framework choices |
| TC-05 | Backward Compatibility | Must maintain compatibility with legacy implementation during transition | Project requirement | Constrains design choices |
| TC-06 | Protocol-First Architecture | Uses Protocol interfaces for component contracts | Architecture decision | Influences implementation patterns |
| TC-07 | MCMC Implementation | MCMC requires specialized implementation considerations | Algorithm requirement | Affects performance and memory usage |
| TC-08 | Model Comparison Framework | Requires framework for systematic model comparison | Architecture decision | Adds complexity to system design |
| TC-09 | Parameter Recovery Validation | Requires framework for parameter recovery validation | Architecture decision | Adds complexity to system design |
| TC-10 | Multiple Inference Algorithms | Must support both SVI and MCMC inference | Architecture decision | Requires flexible model design |

## Computational Resource Constraints {#sec-computational-resource-constraints}

| Constraint ID | Constraint Name | Description | Source | Impact |
|---------------|-----------------|-------------|--------|--------|
| RC-01 | Memory Usage (SVI) | Large single-cell datasets can exceed available memory with SVI | Hardware limitations | Requires efficient memory management |
| RC-02 | Memory Usage (MCMC) | MCMC requires more memory than SVI for storing chains | Hardware limitations | Limits dataset size for MCMC inference |
| RC-03 | SVI Inference Speed | Variational inference can be computationally intensive | Algorithm complexity | Affects user experience and scalability |
| RC-04 | MCMC Inference Speed | MCMC is typically slower than SVI | Algorithm complexity | Limits applicability to very large datasets |
| RC-05 | Parallelization | Need for efficient parallelization across cells, genes, and chains | Performance requirements | Influences algorithm design |
| RC-06 | GPU Acceleration | Should leverage GPU acceleration when available | Performance requirements | Affects implementation choices |
| RC-07 | Deployment Environment | Must run in standard computational biology environments | User environment | Limits dependency choices |
| RC-08 | Parameter Recovery Computation | Parameter recovery validation requires significant computation | Validation requirements | May require high-performance computing resources |
| RC-09 | Model Comparison Computation | Model comparison across multiple configurations is computationally intensive | Validation requirements | May require high-performance computing resources |
| RC-10 | Storage Requirements | Storing multiple models, validation results, and comparison metrics requires significant storage | Hardware limitations | Requires efficient storage management |

# Deployment Requirements {#sec-deployment-requirements}

## Installation Environment {#sec-installation-environment}

| Requirement ID | Requirement Name | Description | Priority |
|----------------|------------------|-------------|----------|
| DE-01 | Python Compatibility | Compatible with Python 3.8+ | High |
| DE-02 | Package Management | Installable via pip and conda | High |
| DE-03 | Dependency Management | Clear specification of dependencies | High |
| DE-04 | Platform Support | Support for Linux, macOS, and Windows | Medium |
| DE-05 | GPU Support | Optional GPU support with clear instructions | Medium |

**Hardware Requirements:**

- CPU: Multi-core x86_64 processor (8+ cores recommended)
- Memory: 16GB+ RAM for SVI (32GB+ recommended)
- Memory: 32GB+ RAM for MCMC (64GB+ recommended)
- Storage: 10GB+ free space for installation
- Storage: 100GB+ free space for model storage and validation results
- GPU: CUDA-compatible GPU with 16GB+ VRAM (optional but recommended)
- GPU: Multiple GPUs for parallel MCMC chains (optional)

**Software Requirements:**

- Python 3.8+
- PyTorch 1.10+
- Pyro 1.8+
- JAX 0.4.0+
- NumPyro 0.11.0+
- torchode 0.1.0+
- diffrax 0.3.0+
- beartype 0.14.0+
- jaxtyping 0.2.20+
- AnnData 0.8+
- Scanpy 1.9+
- NumPy 1.20+
- SciPy 1.7+

**Network Requirements:**

- Internet connection for installation
- No ongoing network requirements for operation

## Deployment Process Requirements {#sec-deployment-process-requirements}

| Requirement ID | Requirement Name | Description | Priority |
|----------------|------------------|-------------|----------|
| DP-01 | Installation Documentation | Clear installation instructions | High |
| DP-02 | Dependency Resolution | Automatic resolution of dependencies | High |
| DP-03 | Version Compatibility | Clear specification of version compatibility | Medium |
| DP-04 | Installation Verification | Verification of successful installation | Medium |
| DP-05 | Containerization | Support for containerized deployment | Low |

## Integration Requirements {#sec-integration-requirements}

| Requirement ID | Requirement Name | Description | Priority |
|----------------|------------------|-------------|----------|
| INT-01 | AnnData Integration | Seamless integration with AnnData objects | High |
| INT-02 | Scanpy Integration | Compatible with Scanpy workflows | High |
| INT-03 | scVelo Integration | Compatible with scVelo for comparison | High |
| INT-04 | Workflow System Integration | Integration with workflow management systems | Medium |
| INT-05 | Visualization Integration | Integration with standard visualization tools | Medium |
| INT-06 | Parameter Recovery Integration | Integration with parameter recovery validation framework | High |
| INT-07 | Model Comparison Integration | Integration with model comparison framework | High |
| INT-08 | MCMC Diagnostics Integration | Integration with MCMC diagnostic tools | Medium |
| INT-09 | JAX/NumPyro Integration | Integration with JAX/NumPyro ecosystem | Medium |
| INT-10 | Synthetic Data Integration | Integration with synthetic data generation tools | Medium |

**Integration Points:**

- AnnData for data exchange
- Scanpy for preprocessing and visualization
- scVelo for comparison and validation
- Workflow systems (Flyte, hydra-zen) for pipeline integration and configuration
- Visualization tools (Matplotlib, Plotly) for result presentation
- Parameter recovery validation framework for model validation
- Model comparison framework for model selection
- MCMC diagnostic tools (ArviZ) for MCMC diagnostics
- JAX/NumPyro ecosystem for alternative implementation
- Synthetic data generation tools for validation

**Data Exchange Formats:**

- AnnData (H5AD) for single-cell data
- HDF5 for model serialization
- CSV/TSV for tabular results
- JSON for configuration and metadata
- PDF/PNG for visualizations
- NetCDF for MCMC samples and diagnostics
- Zarr for large dataset storage
- Parquet for validation results

# Development Requirements {#sec-development-requirements}

## Development Environment {#sec-development-environment}

| Requirement ID | Requirement Name | Description | Priority |
|----------------|------------------|-------------|----------|
| DEV-01 | Version Control | Git-based version control | High |
| DEV-02 | Issue Tracking | Issue tracking system | High |
| DEV-03 | Continuous Integration | Automated testing and integration | High |
| DEV-04 | Code Quality | Code quality checks and linting | Medium |
| DEV-05 | Development Documentation | Documentation for developers | Medium |

**Development Tools:**

- Git for version control
- GitHub for collaboration and issue tracking
- Poetry for dependency management
- Pytest for testing
- Black for code formatting
- Mypy and Beartype for type checking
- Sphinx/Quarto for documentation

**Version Control Requirements:**

- Feature branch workflow
- Pull request reviews
- Semantic versioning
- Conventional commits

**Continuous Integration/Deployment Requirements:**

- Automated testing on pull requests
- Test coverage reporting
- Documentation building
- Package publishing

## Documentation Requirements {#sec-documentation-requirements}

| Requirement ID | Requirement Name | Description | Priority |
|----------------|------------------|-------------|----------|
| DOC-01 | API Documentation | Comprehensive API documentation | High |
| DOC-02 | User Guide | User guide with examples | High |
| DOC-03 | Tutorials | Step-by-step tutorials | High |
| DOC-04 | Architecture Documentation | Documentation of system architecture | Medium |
| DOC-05 | Development Guide | Guide for contributors | Medium |

**Documentation Types:**

- API reference
- User guide
- Tutorials
- Architecture documentation
- Development guide
- Release notes

**Documentation Format:**

- Quarto for web documentation
- Markdown for GitHub documentation
- Docstrings for API documentation
- Jupyter notebooks for tutorials
- Mermaid diagrams for architecture

## Testing and Validation Requirements {#sec-testing-and-validation-requirements}

| Requirement ID | Requirement Name | Description | Priority |
|----------------|------------------|-------------|----------|
| TEST-01 | Unit Testing | Testing of individual components | High |
| TEST-02 | Integration Testing | Testing of component interactions | High |
| TEST-03 | Validation Testing | Validation against ground truth | High |
| TEST-04 | Performance Testing | Testing of performance characteristics | Medium |
| TEST-05 | Compatibility Testing | Testing of ecosystem compatibility | Medium |
| TEST-06 | Parameter Recovery Testing | Testing of parameter recovery capabilities | High |
| TEST-07 | Model Comparison Testing | Testing of model comparison framework | High |
| TEST-08 | MCMC Convergence Testing | Testing of MCMC convergence diagnostics | Medium |
| TEST-09 | Inference Algorithm Testing | Testing of different inference algorithms | Medium |
| TEST-10 | Empirical Identifiability Testing | Testing of parameter identifiability | High |

**Testing Approaches:**

- Unit Testing: Pytest for individual components
- Integration Testing: Pytest-BDD for component interactions
- System Testing: End-to-end workflows
- Performance Testing: Benchmarking against reference datasets
- Validation Testing: Comparison with ground truth and legacy implementation
- Parameter Recovery Testing: Synthetic data with known parameters
- Model Comparison Testing: Comparison of different model architectures
- MCMC Convergence Testing: R-hat statistics and effective sample size
- Inference Algorithm Testing: Comparison of SVI and MCMC results
- Empirical Identifiability Testing: Parameter recovery across sample sizes and noise levels

**Test Environment Requirements:**

- Automated test environment
- Synthetic test data with known ground truth
- Benchmark datasets
- Performance monitoring tools
- Continuous integration system
- Parameter recovery validation framework
- Model comparison framework
- MCMC diagnostic tools
- Multiple inference algorithm implementations
- Synthetic data generation tools

# Risk Assessment {#sec-risk-assessment}

## Technical Risks {#sec-technical-risks}

| Risk ID | Risk Description | Affected Requirements | Probability | Impact | Risk Exposure |
|---------|------------------|----------------------|-------------|--------|---------------|
| TR-01 | Performance issues with large datasets | PERF-01, PERF-02, PERF-03 | Medium | High | High |
| TR-02 | Numerical instability in parameter inference | ACC-01, ACC-03, REL-02 | Medium | High | High |
| TR-03 | Compatibility issues with ecosystem tools | INT-01, INT-02, INT-03 | Low | Medium | Low |
| TR-04 | Difficulty maintaining both PyTorch and JAX implementations | TC-02, TC-04, DEV-03 | Medium | Medium | Medium |
| TR-05 | Dependency conflicts in deployment environments | DE-01, DE-02, DE-03 | Low | Medium | Low |
| TR-06 | MCMC convergence issues with complex models | REL-02, ACC-07, PERF-02 | Medium | High | High |
| TR-07 | Parameter recovery failure for certain model configurations | ACC-06, TEST-06, SC-09 | Medium | High | High |
| TR-08 | Model comparison metrics not aligning with biological relevance | SC-06, ACC-05, TEST-07 | Medium | Medium | Medium |
| TR-09 | Resource constraints for comprehensive validation | RC-08, RC-09, PERF-08 | Medium | Medium | Medium |
| TR-10 | Empirical identifiability issues with real datasets | SC-03, SC-09, ACC-06 | High | High | High |

## Scientific Risks {#sec-scientific-risks}

| Risk ID | Risk Description | Affected Requirements | Probability | Impact | Risk Exposure |
|---------|------------------|----------------------|-------------|--------|---------------|
| SR-01 | Model assumptions not valid for all biological systems | SC-01, SC-02, ACC-02 | High | Medium | High |
| SR-02 | Parameter identifiability issues | SC-03, ACC-01, ACC-03 | Medium | High | High |
| SR-03 | Data quality issues affecting results | SC-04, ACC-02, REL-01 | Medium | High | High |
| SR-04 | Difficulty interpreting uncertainty estimates | USE-04, ACC-03 | Medium | Medium | Medium |
| SR-05 | Challenges in validating results without ground truth | ACC-02, TEST-03 | High | Medium | High |
| SR-06 | Synthetic data not capturing real data complexity | SC-08, TEST-06, ACC-06 | Medium | High | High |
| SR-07 | Model comparison selecting mathematically optimal but biologically irrelevant models | SC-06, ACC-05, CAP-07 | Medium | High | High |
| SR-08 | Different inference algorithms leading to different biological interpretations | SC-07, ACC-07, CAP-02 | Medium | Medium | Medium |
| SR-09 | Empirical identifiability varying across biological contexts | SC-09, ACC-06, TEST-10 | High | Medium | High |
| SR-10 | Model misspecification leading to incorrect biological conclusions | SC-10, ACC-05, CAP-07 | Medium | High | High |

## Mitigation Strategies {#sec-mitigation-strategies}

| Risk ID | Mitigation Strategy | Responsible Party | Monitoring Approach |
|---------|---------------------|-------------------|---------------------|
| TR-01 | Implement data chunking and streaming for large datasets | Development Team | Performance benchmarking |
| TR-02 | Use robust initialization and regularization techniques | Research Team | Numerical stability testing |
| TR-03 | Maintain compatibility layer and comprehensive integration tests | Development Team | Ecosystem compatibility testing |
| TR-04 | Focus on modular design with shared interfaces | Architecture Team | Code quality metrics |
| TR-05 | Use containerization and environment specification | DevOps Team | Deployment testing |
| TR-06 | Implement robust MCMC diagnostics and adaptive sampling | Development Team | MCMC convergence metrics |
| TR-07 | Validate parameter recovery across multiple model configurations | Research Team | Parameter recovery metrics |
| TR-08 | Develop biologically relevant model comparison metrics | Research Team | Validation against benchmark datasets |
| TR-09 | Implement efficient validation workflows and distributed computing | DevOps Team | Resource monitoring |
| TR-10 | Develop empirical identifiability assessment framework | Research Team | Identifiability metrics |
| SR-01 | Develop multiple model variants for different biological contexts | Research Team | Model comparison on diverse datasets |
| SR-02 | Use informative priors and sensitivity analysis | Research Team | Parameter recovery testing |
| SR-03 | Implement robust preprocessing and quality control | Data Team | Data quality metrics |
| SR-04 | Develop intuitive visualizations for uncertainty | UX Team | User feedback |
| SR-05 | Use synthetic data and cross-validation for validation | Validation Team | Validation metrics |
| SR-06 | Develop realistic synthetic data generation with biological variability | Research Team | Comparison with real data distributions |
| SR-07 | Incorporate domain knowledge into model selection criteria | Research Team | Expert validation of model selection |
| SR-08 | Compare inference algorithms on benchmark datasets | Research Team | Consistency of biological interpretations |
| SR-09 | Validate identifiability across multiple biological contexts | Research Team | Context-specific identifiability metrics |
| SR-10 | Implement model misspecification tests | Research Team | Posterior predictive checks |

# Glossary {#sec-glossary}

## Domain Terminology {#sec-domain-terminology}

| Term | Definition | Source |
|------|------------|--------|
| RNA Velocity | The time derivative of gene expression, representing the rate of change of mRNA abundance | La Manno et al., 2018 |
| Spliced mRNA | Mature messenger RNA with introns removed | Molecular biology |
| Unspliced mRNA | Precursor messenger RNA containing introns | Molecular biology |
| Transcription | Process of creating RNA from a DNA template | Molecular biology |
| Splicing | Process of removing introns from pre-mRNA | Molecular biology |
| Degradation | Process of breaking down mRNA | Molecular biology |
| Cell Fate | The developmental trajectory or end state of a cell | Developmental biology |
| Trajectory Inference | Computational methods to order cells along developmental paths | Computational biology |
| Pseudotime | Computational ordering of cells based on their progression through a biological process | Computational biology |
| Parameter Recovery | Validation approach to assess if a model can recover known parameters from synthetic data | Statistical modeling |
| Empirical Identifiability | The ability to uniquely determine parameter values from observed data in practice | Statistical theory |
| Model Comparison | Systematic comparison of different model architectures and inference algorithms | Statistical modeling |
| Model Selection | Process of choosing the most appropriate model based on validation metrics | Statistical modeling |
| Synthetic Data | Artificially generated data with known parameters for validation purposes | Computational statistics |

## Technical Terminology {#sec-technical-terminology}

| Term | Definition | Source |
|------|------------|--------|
| AnnData | Annotated data format for single-cell genomics | AnnData documentation |
| Pyro | Probabilistic programming language built on PyTorch | Pyro documentation |
| Variational Inference | Method for approximating posterior distributions | Statistical learning theory |
| ELBO | Evidence Lower Bound, objective function in variational inference | Statistical learning theory |
| Markov Chain Monte Carlo | Sampling-based method for Bayesian inference | Statistical learning theory |
| No U-Turn Sampler | Adaptive variant of Hamiltonian Monte Carlo for MCMC | Statistical learning theory |
| Credible Interval | Bayesian equivalent of confidence interval, derived from posterior distribution | Statistical learning theory |
| R-hat Statistic | Diagnostic for MCMC convergence comparing within and between chain variance | Statistical learning theory |
| Effective Sample Size | Measure of the effective number of independent samples from MCMC | Statistical learning theory |
| Bayes Factor | Ratio of marginal likelihoods used for Bayesian model comparison | Statistical learning theory |
| Protocol | Interface definition in Python typing | Python typing documentation |
| Modular Architecture | Design approach based on interchangeable components | Software engineering |
| JAX | Numerical computing library with automatic differentiation | JAX documentation |
| NumPyro | Probabilistic programming library built on JAX | NumPyro documentation |
| torchode | Numerical ODE solver for PyTorch | torchode documentation |
| diffrax | Numerical differential equation solver for JAX | diffrax documentation |
| Railway-Oriented Programming | Programming pattern for explicit error handling | Functional programming |
| Parameter Recovery Validation | Framework for validating model parameter recovery | Statistical modeling |
| Model Comparison Framework | Framework for systematic model comparison | Statistical modeling |

## Acronyms {#sec-acronyms}

| Acronym | Definition |
|---------|------------|
| RNA | Ribonucleic Acid |
| mRNA | Messenger RNA |
| scRNA-seq | Single-cell RNA sequencing |
| SVI | Stochastic Variational Inference |
| MCMC | Markov Chain Monte Carlo |
| NUTS | No U-Turn Sampler |
| HMC | Hamiltonian Monte Carlo |
| ELBO | Evidence Lower Bound |
| ODE | Ordinary Differential Equation |
| SDE | Stochastic Differential Equation |
| PRV | Parameter Recovery Validation |
| MCF | Model Comparison Framework |
| ESS | Effective Sample Size |
| CI | Credible Interval |
| BF | Bayes Factor |
| GPU | Graphics Processing Unit |
| API | Application Programming Interface |
| CI/CD | Continuous Integration/Continuous Deployment |
| BDD | Behavior-Driven Development |
